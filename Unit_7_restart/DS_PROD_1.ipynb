{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e52c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17b321a",
   "metadata": {},
   "source": [
    "#### ШАГ №1\n",
    "Обучим модель линейной регрессии на встроенном датасете о диабете — `Diabetes dataset`.\n",
    "\n",
    "В данном датасете представлены десять исходных признаков: \n",
    "* возраст, \n",
    "* пол, \n",
    "* индекс массы тела, \n",
    "* среднее артериальное давление \n",
    "* шесть измерений сыворотки крови   \n",
    "\n",
    "Они были получены для каждого из 442 пациентов с сахарным диабетом.   \n",
    "\n",
    "Интерес представляет количественный показатель прогресса заболевания, замеренный через год после исходного измерения. Тип задачи — регрессия.\n",
    "\n",
    ">Примечание. Для простоты и наглядности мы опустим процесс предобработки данных, разведывательный анализ, разделение выборки на обучающую и тестовую, валидацию модели и подбор гиперпараметров, так как для наших целей это неважно. Мы уверены, что вы уже способны произвести эти этапы самостоятельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "098fb938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Загружаем датасет о диабете\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "# Инициализируем модель линейной регрессии\n",
    "regressor = LinearRegression()\n",
    "# Обучаем модель\n",
    "regressor.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d298c086",
   "metadata": {},
   "source": [
    "В результате выполнения кода получился объект класса `LinearRegression`, на который ссылается переменная `regressor`. При этом атрибуты объекта (веса модели линейной регрессии) были сформированы во время обучения. То есть объект `regressor` теперь является обученной моделью.\n",
    "\n",
    "#### ШАГ №2\n",
    "\n",
    "Далее, когда мы получили обученную модель, нам необходимо сериализовать её, превратив **объект Python** в поток байтов. Для этого импортируем модуль `pickle` и воспользуемся функцией `dumps()`, в которую нужно передать **объект Python**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "004edd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "<class 'sklearn.linear_model._base.LinearRegression'>\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Производим сериализацию обученной модели\n",
    "model = pickle.dumps(regressor)\n",
    "\n",
    "print(type(model))\n",
    "print(type(regressor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d423b67f",
   "metadata": {},
   "source": [
    "Как видим, мы создали объект `model` типа `bytes`.\n",
    "\n",
    "#### ШАГ №3\n",
    "\n",
    "Давайте попробуем восстановить (десериализовать) **объект Python**. Для этого в модуле `pickle` есть функция `loads()`, в которую нужно передать сериализованный объект (поток байтов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06a4831b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Производим десериализацию\n",
    "regressor_from_bytes = pickle.loads(model)\n",
    "regressor_from_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aaef200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor_from_bytes == regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5778d9d8",
   "metadata": {},
   "source": [
    "В результате десериализации мы смогли восстановить исходный объект (модель).\n",
    "\n",
    "#### ШАГ №4\n",
    "\n",
    "Сохраним сериализованный объект прямо в файл. Для этого в `pickle` есть функция `dump()` (без s на конце). В неё необходимо передать имя файла или ссылку на открытый файл. Файл назовём `myfile`, его расширение — `pkl` (формат данных `pickle`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4dacaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Производим сериализацию и записываем результат в файл формата pkl\n",
    "with open('myfile.pkl', 'wb') as output:\n",
    "    pickle.dump(regressor, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178e87da",
   "metadata": {},
   "source": [
    "Теперь у нас есть бинарный файл с готовой моделью, и мы можем передать его, например, ML-инженерам, которые будут заниматься деплоем модели на сервер.\n",
    "\n",
    "#### ШАГ №5\n",
    "\n",
    "Посмотрим на код, который восстанавливает (десериализует) обученную модель из файла **myfile.pkl**. Для этого в `pickle` есть функция `load()` (без s на конце). В неё необходимо передать имя файла или ссылку на открытый файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a48cbc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Производим десериализацию и извлекаем модель из файла формата pkl\n",
    "with open('myfile.pkl', 'rb') as pkl_file:\n",
    "    regressor_from_file = pickle.load(pkl_file)\n",
    "\n",
    "regressor_from_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419c97e",
   "metadata": {},
   "source": [
    "#### ШАГ №6\n",
    "\n",
    "Убедимся, что методы и результаты предсказаний обученной модели и модели, загруженной из файла, совпадают:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28d05065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверяем, что все элементы массивов предсказаний совпадают между собой\n",
    "all(regressor.predict(X) == regressor_from_bytes.predict(X)),\\\n",
    "all(regressor.predict(X) == regressor_from_file.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7db1d8",
   "metadata": {},
   "source": [
    "Как видите, исходная и восстановленная из байтов и файла модели дают одинаковые предсказания. Это значит, что теперь мы можем импортировать наши обученные модели в любое Python-приложение и пользоваться ими, минуя этап обучения и все этапы, предшествующие ему.\n",
    "\n",
    "#### ОГРАНИЧЕНИЯ\n",
    "\n",
    "Как мы упоминали, у pickle есть ограничения. Например, мы не можем сериализовать лямбда-функции. Давайте посмотрим, что нам вернёт следующий код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a4d30a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function <lambda> at 0x000002821A03BB88>: attribute lookup <lambda> on __main__ failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9340\\517191026.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmy_lambda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'my_lambda.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_lambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <function <lambda> at 0x000002821A03BB88>: attribute lookup <lambda> on __main__ failed"
     ]
    }
   ],
   "source": [
    "my_lambda = lambda x: x*2\n",
    "with open('my_lambda.pkl', 'wb') as output:\n",
    "    pickle.dump(my_lambda, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e216f2",
   "metadata": {},
   "source": [
    "### СОХРАНЕНИЕ ПАЙПЛАЙНА\n",
    "\n",
    "Ранее мы посмотрели простейший пример сериализации готовой модели.\n",
    "\n",
    "У вас мог возникнуть вопрос: что делать, если перед подачей данных в модель их необходимо предобработать, например произвести стандартизацию, исключить неинформативные признаки? Неужели придётся прописывать все эти шаги в коде инференса модели? А что если вопросами инференса занимаются совершенно другие специалисты, которые вообще ничего не знают о машинном обучении и не умеют производить предобработку данных?\n",
    "\n",
    "Конечно, мы должны передать результаты в таком виде, чтобы ими можно было воспользоваться без лишних манипуляций.\n",
    "\n",
    "Мы уже упоминали, что `pickl`e работает с любыми объектами Python. Поэтому для сохранения может быть доступна не просто обученная модель, но и целый пайплайн, включающий предобработку данных.\n",
    "\n",
    "Например, мы хотим сериализовать пайплайн, который включает в себя **min-max-нормализацию** и **отбор пяти наиболее важных факторов на основе корреляции Пирсона**. Полученные в результате данные отправляются на вход модели **линейной регрессии**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e14972ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('Scaling', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       "                ('FeatureSelection',\n",
       "                 SelectKBest(k=5,\n",
       "                             score_func=<function f_regression at 0x0000028227012A68>)),\n",
       "                ('Linear',\n",
       "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "                                  normalize=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загружаем датасет о диабете\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "\n",
    "# Создаём пайплайн, который включает нормализацию, отбор признаков и обучение модели\n",
    "pipe = Pipeline([  \n",
    "  ('Scaling', MinMaxScaler()),\n",
    "  ('FeatureSelection', SelectKBest(f_regression, k=5)),\n",
    "  ('Linear', LinearRegression())\n",
    "  ])\n",
    "\n",
    "# Обучаем пайплайн\n",
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0a362e",
   "metadata": {},
   "source": [
    "Пайплайн обучен. Давайте сохраним его в файл с помощью pickle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97151bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сериализуем pipeline и записываем результат в файл\n",
    "with open('my_pipeline.pkl', 'wb') as output:\n",
    "    pickle.dump(pipe, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd35440",
   "metadata": {},
   "source": [
    "Если сериализация завершилась успешно, то при инференсе модели мы сможем восстановить её из файла:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a81e5653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Десериализуем pipeline из файла\n",
    "with open('my_pipeline.pkl', 'rb') as pkl_file:\n",
    "    loaded_pipe = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a228c8",
   "metadata": {},
   "source": [
    "Проверим, что результаты исходного и десериализованного пайплайнов и идентичны:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98f11ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Сравниваем предсказания исходного и восстановленного пайплайнов\n",
    "print(all(pipe.predict(X) == loaded_pipe.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60862b22",
   "metadata": {},
   "source": [
    "Примечание. Если мы хотим сохранять сериализованные пайплайны в виде потока байтов, нужно использовать функции `dumps()` и `loads()`, а не `dump()` и `load()`.\n",
    "\n",
    "Однако в процессе предобработки могут возникнуть шаги, которые нельзя реализовать стандартными методами `sklearn`. Например, для решения многих задач в нашем курсе мы часто использовали feature engineering, чтобы повысить качество работы моделей. Как встроить этот шаг в исходный пайплайн?\n",
    "\n",
    "Для этого в `sklear`n можно организовать так называемые кастомные трансформеры. Такой трансформер должен наследоваться от двух классов: `TransformerMixin` и `BaseEstimator`.\n",
    "\n",
    "Посмотрим на шаблон кастомного трансформера:\n",
    "```PYTHON\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "class MyTransformer(TransformerMixin, BaseEstimator):\n",
    "    '''Шаблон кастомного трансформера'''\n",
    " \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Здесь прописывается инициализация параметров, не зависящих от данных.\n",
    "        '''\n",
    "        pass\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        '''\n",
    "        Здесь прописывается «обучение» трансформера.\n",
    "        Вычисляются необходимые для работы трансформера параметры (если они нужны).\n",
    "        '''\n",
    "\n",
    "        return self\n",
    " \n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        Здесь прописываются действия с данными.\n",
    "        '''\n",
    "        return X\n",
    "```\n",
    "\n",
    "У трансформера должно быть три обязательных метода:\n",
    "\n",
    "* `__init__()` — метод, который вызывается при создании объекта данного класса. Он предназначен для инициализации исходных параметров.\n",
    "Например, у трансформера для создания полиномиальных признаков **PolynomialFeatures** из **sklearn** в методе `__init__()` параметр degree задаёт степень полинома.\n",
    "* `fit()` — метод, который вызывается для «обучения» трансформера. Он должен возвращать ссылку на сам объект (self).\n",
    "Например, в трансформере `StandardScaler` в методе `fit()` прописано вычисление среднего значения и стандартного отклонения в каждом столбце таблицы, переданной в качестве параметра метода `fit()`.\n",
    "* `transform()` — метод, который трансформирует приходящие на вход данные. Он должен возвращать преобразованный массив данных.\n",
    "Например, при вызове метода `transform()` у `StandardScaler` из `*sklearn` внутри происходит преобразование — вычитание из каждого столбца среднего и деление результата на стандартное отклонение. Причём среднее и стандартное отклонение вычисляются заранее в методе `fit()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9e4037",
   "metadata": {},
   "source": [
    "Наш трансформер пока что ничего не делает. Предположим, мы хотим генерировать в данных новый признак, который является простым произведением первых трёх столбцов таблицы. Давайте пропишем в методе `transform()` эти действия.\n",
    "\n",
    "Для работы такого трансформера нужны только исходные данные без дополнительных параметров, поэтому методы `__init__()` и `fit()`* остаются без изменений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67325513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class MyTransformer(TransformerMixin, BaseEstimator):\n",
    "    '''Шаблон кастомного трансформера'''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''Здесь прописывается инициализация параметров, не зависящих от данных.'''\n",
    "        pass\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        '''\n",
    "        Здесь прописывается «обучение» трансформера.\n",
    "        Вычисляются необходимые для работы трансформера параметры (если они нужны).\n",
    "        '''\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''Здесь прописываются действия с данными.'''\n",
    "        # Создаём новый столбец как произведение первых трёх\n",
    "        new_column = X[:, 0] * X[:, 1] * X[:, 2]\n",
    "        # Для добавления столбца в массив нужно изменить его размер на (n_rows, 1)\n",
    "        new_column = new_column.reshape(X.shape[0], 1)\n",
    "        # Добавляем столбец в матрицу измерений\n",
    "        X = np.append(X, new_column, axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435e2d0a",
   "metadata": {},
   "source": [
    "Посмотрим, как работает наш кастомный трансформер. Создадим объект трансформера, вызовем метод transform и посмотрим на результирующий размер таблицы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c28481a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before transform: (442, 10)\n",
      "Shape after transform: (442, 11)\n"
     ]
    }
   ],
   "source": [
    "# Инициализируем объект класса MyTransformer (вызывается метод __init__)\n",
    "custom_transformer = MyTransformer()\n",
    "# Чисто формально вызываем метод fit, но у нас он ничего не делает\n",
    "custom_transformer.fit(X)\n",
    "# Трансформируем исходные данные (вызывается метод transform)\n",
    "X_transformed = custom_transformer.transform(X)\n",
    "print('Shape before transform: {}'.format(X.shape))\n",
    "print('Shape after transform: {}'.format(X_transformed.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd48afd3",
   "metadata": {},
   "source": [
    "Видно, что в результате трансформации в исходную матрицу наблюдений добавился новый столбец.\n",
    "\n",
    "Теперь давайте встроим этот трансформер в сам пайплайн — для этого достаточно добавить новый шаг в пайплайн."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24a34ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('FeatureEngineering', MyTransformer()),\n",
       "                ('Scaling', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       "                ('FeatureSelection',\n",
       "                 SelectKBest(k=5,\n",
       "                             score_func=<function f_regression at 0x0000028227012A68>)),\n",
       "                ('Linear',\n",
       "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "                                  normalize=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создаём пайплайн, который включает Feature Engineering, нормализацию, отбор признаков и обучение модели\n",
    "pipe = Pipeline([  \n",
    "  ('FeatureEngineering', MyTransformer()),\n",
    "  ('Scaling', MinMaxScaler()),\n",
    "  ('FeatureSelection', SelectKBest(f_regression, k=5)),\n",
    "  ('Linear', LinearRegression())\n",
    "  ])\n",
    "\n",
    "# Обучаем пайплайн\n",
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21add88",
   "metadata": {},
   "source": [
    "Наконец можно сериализовать полученный pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b68ce271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сериализуем pipeline и записываем результат в файл\n",
    "with open('my_new_pipeline.pkl', 'wb') as output:\n",
    "    pickle.dump(pipe, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fe11d8",
   "metadata": {},
   "source": [
    "> Теперь мы можем передать пайплайн и воспользоваться им для инференса, предварительно произведя десериализацию."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68385c2d",
   "metadata": {},
   "source": [
    "### Задание 2.5\n",
    "Десериализуйте полученный pipeline с добавленным в него кастомной трансформации из файла. Затем предскажите значение целевой переменной для наблюдения, которое описывается следующим вектором:\n",
    "```python\n",
    "features = np.array([[ 0.00538306, -0.04464164,  0.05954058, -0.05616605,  0.02457414, 0.05286081, -0.04340085,  0.05091436, -0.00421986, -0.03007245]])\n",
    "```\n",
    "В поле для ответа введите предсказанное значение целевой переменной, округлённое до **целого числа**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0085a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array([[ 0.00538306, -0.04464164,  0.05954058, -0.05616605,  0.02457414, \n",
    "                      0.05286081, -0.04340085,  0.05091436, -0.00421986, -0.03007245]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58a9e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('my_new_pipeline.pkl', 'rb') as pkl_file:\n",
    "    pipeline_from_file = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2e8f3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = pipeline_from_file.predict(features)[0]\n",
    "round(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec2906",
   "metadata": {},
   "source": [
    "### БИБЛИОТЕКА JOBLIB\n",
    "\n",
    "Как мы видим, `pickle` прекрасно справляется со своей задачей: мы можем сериализовать и восстанавливать любые **Python-объекты**, включая модели и даже пайплайны. Однако иногда массивы данных, на которых обучаются модели, бывают настолько большими, что после загрузки из `pickle` невозможно восстановить объект полностью.\n",
    "\n",
    "В таких случаях вместо `pickle` лучше использовать библиотеку `joblib`. Этот модуль более эффективен и надёжен для работы с объектами, которые содержат большие массивы данных. Пожалуй, единственный минус этого модуля в том, что он может «консервировать» только в файл, поэтому вы не сможете получить объект в виде бинарной строки и работать с ним. В модуле попросту отсутствуют методы для работы с бинарной строкой. Формат файлов для сохранения — `.joblib`.\n",
    "\n",
    "В остальном работа с `joblib` полностью идентична работе с `pickle`: после обучения модели производим сериализацию с помощью функции `dump()`, а в коде самого приложения, где нужно использовать модель, выполняем десериализацию с помощью функции `load()`. В каждую из этих функций необходимо передать путь до файла для записи и чтения соответственно.\n",
    "\n",
    "Для иллюстрации работы сохраним полученную линейную регрессию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afe5ede1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['regr.joblib']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Загружаем датасет о диабете\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "# Обучаем модель линейной регрессии\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X, y)\n",
    "# Производим сериализацию и сохраняем результат в файл формата .joblib\n",
    "joblib.dump(regressor, 'regr.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5968245",
   "metadata": {},
   "source": [
    "Загрузим файл заново (загрузка может быть произведена в другом файле с кодом):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0429a53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Десериализуем модель из файла\n",
    "clf_from_jobliv = joblib.load('regr.joblib') \n",
    "# Сравниваем предсказания\n",
    "all(regressor.predict(X) == clf_from_jobliv.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8bd1ac",
   "metadata": {},
   "source": [
    "## 3. Практика: pickle\n",
    "Ваш коллега Василий обучил модель и теперь просит вас проверить её на ваших данных. Он присылает вам **pickle-файл**. Загрузите модель, используя модуль `pickle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e95e8720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret word: skillfactory\n",
      "how is this possible? answer is here: https://youtu.be/xm-A-h9QkXg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GANSOR-PC\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator LinearRegression from version 0.22.2.post1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "with open('models/model.pkl', 'rb') as pkl_file:\n",
    "    vasil_model = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eb9a43",
   "metadata": {},
   "source": [
    "## Задание 3.2\n",
    "Проверьте, объект какого типа получился. Какую модель вам прислал коллега?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2a78958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.linear_model._base.LinearRegression"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vasil_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2e3db5",
   "metadata": {},
   "source": [
    "### Задание 3.3\n",
    "Теперь необходимо применить модель. Сделайте предсказание для следующего набора фичей: `[1, 1, 1, 0.661212487096872]`. Введите результат, предварительно округлив его до трёх знаков после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "513c92e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.666"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 1, 1, 0.661212487096872]])\n",
    "ans = vasil_model.predict(x)[0]\n",
    "round(ans,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff761f6e",
   "metadata": {},
   "source": [
    "У присланной вам модели есть два поля (атрибута) с именами a и b. Создайте из них словарь с такими же именами ключей и значениями, а затем сохраните его в файл с помощью модуля `pickle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c1eda6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_33_dict = {'a': vasil_model.a, 'b': vasil_model.b}\n",
    "\n",
    "with open('task_33_dict.pkl', 'wb') as pkl_file:\n",
    "    pickle.dump(task_33_dict, pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ed074",
   "metadata": {},
   "source": [
    "### Задание 3.4\n",
    "Чтобы вы могли проверить правильность решения задания, мы создали специальный проверочный скрипт. Скачайте его здесь.\n",
    "\n",
    "Сохраните его рядом с вашим pickle-файлом (в той же папке) и запустите, передав первым аргументом имя файла. Если вы всё сделали правильно, на экран выведется ответ для следующего задания.\n",
    ">В ячейке Jupyter Notebook:    \n",
    "```!python hw1_check_ol.py имя_pickle_файла.pkl```    \n",
    "\n",
    ">В терминале:\n",
    ">```cd имя_папки_со_скриптом   \n",
    ">python hw1_check_ol.py имя_pickle_файла.pkl```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62d39b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('secret code 2:', '3c508')\n"
     ]
    }
   ],
   "source": [
    "!python hw1_check_ol.py task_33_dict.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec26ea37",
   "metadata": {},
   "source": [
    "## 4. Сохранение и загрузка моделей: PMML и ONNX-ML\n",
    ">Среда или требования к инференсу модели для вашего проекта могут быть устроены так, что потребуют реализации на языке программирования, отличном от Python. Например, если компания разрабатывает десктопное приложение, то для внедрения модели её потребуется «перевести» на Java или C++. Как это сделать?\n",
    "\n",
    "### PREDICTIVE MODEL MARKUP LANGUAGE\n",
    "В таких случаях используется генерация файла формата PMML (Predictive Model Markup Language).\n",
    "\n",
    "PMML — это XML-диалект, который применяется для описания статистических и DS-моделей. PMML-совместимые приложения позволяют легко обмениваться моделями данных между собой. Разработка и внедрение PMML осуществляется IT-консорциумом Data Mining Group.\n",
    "\n",
    "Подробнее с PMML можно ознакомиться на официальном сайте.\n",
    "\n",
    "К сожалению, далеко не все библиотеки для машинного обучения (в том числе sklearn) поддерживают возможность сохранения обученной модели в указанном формате. Однако для этого можно использовать сторонние библиотеки, и одной из самых популярных является `Nyoka`.\n",
    "\n",
    "Среда или требования к инференсу модели для вашего проекта могут быть устроены так, что потребуют реализации на языке программирования, отличном от Python. Например, если компания разрабатывает десктопное приложение, то для внедрения модели её потребуется «перевести» на Java или C++. Как это сделать?\n",
    "\n",
    "PREDICTIVE MODEL MARKUP LANGUAGE\n",
    "\n",
    "В таких случаях используется генерация файла формата PMML (Predictive Model Markup Language).\n",
    "\n",
    "PMML — это XML-диалект, который применяется для описания статистических и DS-моделей. PMML-совместимые приложения позволяют легко обмениваться моделями данных между собой. Разработка и внедрение PMML осуществляется IT-консорциумом Data Mining Group.\n",
    "\n",
    "Подробнее с PMML можно ознакомиться на официальном сайте.\n",
    "\n",
    "К сожалению, далеко не все библиотеки для машинного обучения (в том числе sklearn) поддерживают возможность сохранения обученной модели в указанном формате. Однако для этого можно использовать сторонние библиотеки, и одной из самых популярных является Nyoka.\n",
    "\n",
    "Давайте сохраним модель из предыдущего блока в формат PMML.\n",
    "\n",
    "Рассмотрим пример работы с библиотекой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8ae914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nyoka import skl_to_pmml\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "cols = load_diabetes()['feature_names']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "pipe = Pipeline([  \n",
    "            ('Scaling', MinMaxScaler()),\n",
    "            ('Linear', LinearRegression())\n",
    "        ])\n",
    "# Обучение пайплайна, включающего линейную модель и нормализацию признаков\n",
    "pipe.fit(X, y)\n",
    "# Сохраним пайплайн в формате pmml в файл pipeline.pmml\n",
    "skl_to_pmml(pipeline=pipe, col_names=cols, pmml_f_name=\"models/pipeline.pmml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7fd622",
   "metadata": {},
   "source": [
    "### OPEN NEURAL NETWORK EXCHANGE\n",
    "\n",
    "В разработке моделей на основе нейронных сетей сегодня наиболее распространён формат ONNX (Open Neural Network Exchange).\n",
    "\n",
    "ONNX (Open Neural Network Exchange) — это открытый стандарт для обеспечения совместимости моделей машинного обучения. Он позволяет разработчикам искусственного интеллекта использовать модели с различными инфраструктурами, инструментами, средами исполнения и компиляторами.\n",
    "\n",
    "Стандарт совместно поддерживается компаниями Microsoft, Amazon, Facebook и другими партнёрами как проект с открытым исходным кодом.\n",
    "\n",
    "Часто стандарт ONNX и его библиотеки используют для конвертации из одного фреймворка в другой (например, из PyTorch в TensorFlow для использования в продакшене). Для конвертации различных фреймворков (не только DL) в формат ONNX и обратно существует ряд библиотек:\n",
    "\n",
    "* ONNX-Tensorflow;\n",
    "* Tensorflow-ONNX;\n",
    "* Keras-ONNX;\n",
    "* Sklearn-ONNX.\n",
    "* и другие.   \n",
    "\n",
    "Также в рамках стандарта ONNX есть инструмент ONNX-runtime. Он служит для ускорения инференса Python-моделей, а также инференса на других языках, например Java, C++."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b065d04f",
   "metadata": {},
   "source": [
    "## 5. Деплой модели. Протоколы сетевого взаимодействия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f41ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e8e6d81",
   "metadata": {},
   "source": [
    "## 7. Пишем сервер на Flask\n",
    "Если бы вы попробовали написать сервер в ***Jupyter Notebook***, то почти сразу поняли бы, что он для этого не предназначен. Как минимум потому, что после запуска вы больше не смогли бы выполнить в ноутбуке ни одной команды — программа сервера переходит в режим прослушки сети и ожидания приходящих запросов. Поэтому разработку нашего веб-сервиса мы будем вести в обычных файлах ***.py***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5accc82e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b88a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2040d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee22c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e244d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
