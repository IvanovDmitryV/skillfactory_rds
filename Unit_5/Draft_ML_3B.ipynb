{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем реализовать классификацию и вычислить разобранные метрики.\n",
    "Для начала подгружаем библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # функция, чтобы разбить данные на трейн и тест\n",
    "from sklearn.linear_model import LogisticRegression # наша модель для классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся встроенным датасетом, который содержит информацию об опухолях груди:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer # подгружаем датасет\n",
    "breast_cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь зададим зависимую и независимые переменные:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = breast_cancer.target ## Наша целевая переменная, 0 — если рака нет, 1 — если есть \n",
    "X = breast_cancer.data # X - признаки, по которым мы будем предсказывать рак "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиваем выборку на обучающую и тестовую и обучаем нашу модель:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.3)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готово! Теперь осталось только вычислить необходимые метрики:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9590643274853801\n",
      "0.9478260869565217\n",
      "0.990909090909091\n",
      "0.9688888888888889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "Y_predicted = model.predict(X_val)\n",
    "print(accuracy_score(Y_val,Y_predicted))\n",
    "print(precision_score(Y_val,Y_predicted))\n",
    "print(recall_score(Y_val,Y_predicted))\n",
    "print(f1_score(Y_val,Y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3B.2.1\n",
    "Вы создали классификатор, который разделяет экономические и политические новости на два разных Telegram-канала, и хотите проверить его качество. За день вышло 15 политических новостей и 20 экономических.  \n",
    "Ваш алгоритм из 15 политических новостей отметил 9 как экономические, а из 20 экономических — 6 как политические.  \n",
    "Найдите метрику $Accuracy$ .  \n",
    "Ответ округлите до сотых. Пример ввода: 5.55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (6+14)/(15+20)\n",
    "round(accuracy,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3B2.2\n",
    "Загрузите встроенный в библиотеку `sklearn` датасет про ирисы с помощью функции `load_iris`. Обучите модель логистической регрессии (random_state=50, размер тестовой выборки 0.3) и укажите полученное значение метрики $Accuracy$.  \n",
    "Ответ округлите до сотых. Пример ввода: 5.55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=50)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(round(accuracy_score(y_pred, y_test),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3B.3. Классификация. Практика\n",
    "У вас есть датасет с параметрами мобильных телефонов. Переменная `price_range` отвечает за то, к какой категории относится телефон: 1 — дорогие, 0 — дешевые.  \n",
    "Ваша задача состоит в том, чтобы наиболее точно научиться классифицировать телефоны по этим двум категориям на основании других параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./Unit_5_data/train_mobile.csv', sep =';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 1\n",
    "Для начала нам надо отобрать признаки, с помощью которых мы будем предсказывать категорию телефона.\n",
    "\n",
    "### Задание 3B.3.1 Отбор признаков\n",
    "Выберите пять признаков, у которых наибольшая взаимосвязь с целевой переменной (с помощью корреляции). Отметьте отобранные признаки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ram\n",
      "battery_power\n",
      "px_width\n",
      "px_height\n",
      "touch_screen\n"
     ]
    }
   ],
   "source": [
    "for feat in abs(data.corr().price_range).sort_values(ascending=False).index.drop('price_range')[:5]: print (feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ШАГ 2\n",
    "Теперь необходимо обучить алгоритм.  Для начала разбейте выборку на тестовую и обучающую, размер тестовой задайте `0.2`. Параметр `random_state=31`. В качестве модели возьмите логистическую регрессию. В качестве предикторов возьмите пять ранее отобранных признаков.\n",
    "\n",
    "Рассчитайте метрику, которая покажет, какая доля телефонов, обозначенных классификатором как дорогие, действительно относится к этой категории. \n",
    "\n",
    "### Задание 3B.3.2 Выбор метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9716"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "data = pd.read_csv('./Unit_5_data/train_mobile.csv', sep =';')\n",
    "\n",
    "columns_to_fit = abs(data.corr().price_range).sort_values(ascending=False).index[1:6]\n",
    "\n",
    "y = data.price_range\n",
    "X = data[columns_to_fit]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=31)\n",
    "\n",
    "model = LogisticRegression(random_state=31)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "precision = precision_score(y_test,y_pred)\n",
    "round(precision,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3B.3.3 Значение метрики\n",
    "Введите полученное значение, округлите до четырех знаков после запятой. Целую и десятичную часть разделите точкой. Пример ввода: 5.5555."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9716"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(precision,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3B.5.3\n",
    "Посчитайте $logloss$ для данных в таблице (без нормализации). Укажите число с точностью до сотых:  \n",
    "\n",
    " | | | | | |\n",
    " |-|-|-|-|-|\n",
    " |Предсказанное значение |0.2|0.8|1|0.6|\n",
    " |Истинное значение      |0|0|1|1 |\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.34, 2.34)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.Series([0,0,1,1])\n",
    "y_pred = pd.Series([0.2,0.8,1,0.6])\n",
    "n = 4\n",
    "\n",
    "logloss = -(y*np.log(y_pred) + (1-y)*np.log(1-y_pred)).sum()\n",
    "\n",
    "round(log_loss(y,y_pred)*n,2), round(logloss,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.223144\n",
       "1    1.609438\n",
       "2         NaN\n",
       "3    0.510826\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(y*np.log(y_pred) + (1-y)*np.log(1-y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3B.5.4\n",
    "Посчитайте $logloss$ для данных в таблице. Необходимо найти среднюю ошибку. Классификация на три класса:\n",
    "\n",
    "| | | | |\n",
    "|-|-|-|-|\n",
    "|Предсказанное значение|0.2|0|0.1|\n",
    "| |0.3|0 |0  |\n",
    "| |0.5|1 |0.9|\n",
    "|Истинное значение     |0|0|1|\n",
    "|  |0|0|0|\n",
    "|  |1|1|0|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.998577424517998, 0.9985774245179969)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.array([0.2,0.3,0.5,0,0,1,0.1,0,0.9]).reshape((3,3))\n",
    "y = np.array([0,0,1,0,0,1,1,0,0]).reshape((3,3))\n",
    "n = 3\n",
    "\n",
    "logloss = -np.nan_to_num(y*np.log(y_pred)).sum()/n\n",
    "\n",
    "log_loss(y,y_pred), logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3B.6. Логистическая регрессия. Практика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error, f1_score, accuracy_score, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае с логистической регрессией мы можем использовать только градиентный спуск, так как нет явного матричного способа найти оптимальные коэффициенты. В качестве функции потерь будем использовать бинарную кросс-энтропию, Log Loss. Она записывается так:\n",
    "\n",
    "$$L=-\\frac{1}{n}(y_i \\log h_{\\theta}(x_i) + (1-y_i) \\log(1-h_{\\theta}(x_i)))$$\n",
    "Градиент ошибки:\n",
    "$$\\nabla L=\\frac{1}{n}\\sum_{i=1}^{n}{(h_{\\theta}(x_i)-y_i)x_i}$$\n",
    "Будем использовать другой датасет с задачей классификации, где нужно определить зарплату меньше и больше определённого значения. Убираем в данных лишние признаки, конвертируем целевой столбец в бинарные значения и нормализуем данные.\n",
    "\n",
    "Реализуем функцию `sigmoid` и функцию, вычисляющую градиент бинарной кросс-энтропии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = pd.read_csv('./Unit_5_data/adult.data',\n",
    "                    names=['age', 'workclass', 'fnlwgt', 'education',\n",
    "                           'education-num', 'marital-status', 'occupation',\n",
    "                           'relationship', 'race', 'sex', 'capital-gain',\n",
    "                           'capital-loss', 'hours-per-week', 'native-country', 'salary'])\n",
    "\n",
    "# Избавиться от лишних признаков\n",
    "adult.drop(['native-country'], axis=1, inplace=True)\n",
    "# Сконвертировать целевой столбец в бинарные значения\n",
    "adult['salary'] = (adult['salary'] != ' <=50K').astype('int32')\n",
    "# Сделать one-hot encoding для некоторых признаков\n",
    "adult = pd.get_dummies(adult, columns=['workclass', 'education', 'marital-status', 'occupation', \n",
    "                                       'relationship', 'race', 'sex'])\n",
    "\n",
    "# Нормализовать нуждающиеся в этом признаки\n",
    "a_features = adult[['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']].values\n",
    "norm_features = (a_features - a_features.mean(axis=0)) / a_features.std(axis=0)\n",
    "adult.loc[:, ['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']] = norm_features\n",
    "\n",
    "# Разбить таблицу данных на матрицы X и y\n",
    "X = adult[list(set(adult.columns) - set(['salary']))].values\n",
    "y = adult['salary'].values\n",
    "\n",
    "# Добавить фиктивный столбец единиц (bias линейной модели)\n",
    "X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])\n",
    "m = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X, theta):\n",
    "    return 1. / (1. + np.exp(-X.dot(theta)))\n",
    "\n",
    "def calc_binary_cross_entropy_grad(X, y, theta):\n",
    "    n = X.shape[0]\n",
    "    grad = 1. / n * X.transpose().dot(sigmoid(X, theta) - y)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def gradient_step(theta, theta_grad, alpha):\n",
    "    return theta - alpha * theta_grad\n",
    "def optimize(X, y, grad_func, start_theta, alpha, n_iters):\n",
    "    theta = start_theta.copy()\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        theta_grad = grad_func(X, y, theta)\n",
    "        theta = gradient_step(theta, theta_grad, alpha)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оптимизировать параметр линейной регрессии theta на всех данных\n",
    "theta = optimize(X, y, calc_binary_cross_entropy_grad, np.ones(m), 1., 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_logisitc_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f'acc = {acc:.2f} F1-score = {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 0.85 F1-score = 0.65\n"
     ]
    }
   ],
   "source": [
    "y_pred = sigmoid(X, theta) > 0.5\n",
    "print_logisitc_metrics(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3B.6.1\n",
    "Постройте модель логистической регрессии при помощи sklearn. Используйте параметры по умолчанию, обучите на всей выборке и посчитайте F1 score.\n",
    "\n",
    "Ответ округлите до сотых. Пример ввода: 5.55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = pd.read_csv('./Unit_5_data/adult.data',\n",
    "                    names=['age', 'workclass', 'fnlwgt', 'education',\n",
    "                           'education-num', 'marital-status', 'occupation',\n",
    "                           'relationship', 'race', 'sex', 'capital-gain',\n",
    "                           'capital-loss', 'hours-per-week', 'native-country', 'salary'])\n",
    "\n",
    "# Избавиться от лишних признаков\n",
    "adult.drop(['native-country'], axis=1, inplace=True)\n",
    "# Сконвертировать целевой столбец в бинарные значения\n",
    "adult['salary'] = (adult['salary'] != ' <=50K').astype('int32')\n",
    "# Сделать one-hot encoding для некоторых признаков\n",
    "adult = pd.get_dummies(adult, columns=['workclass', 'education', 'marital-status', 'occupation', \n",
    "                                       'relationship', 'race', 'sex'])\n",
    "\n",
    "# Нормализовать нуждающиеся в этом признаки\n",
    "a_features = adult[['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']].values\n",
    "norm_features = (a_features - a_features.mean(axis=0)) / a_features.std(axis=0)\n",
    "adult.loc[:, ['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']] = norm_features\n",
    "\n",
    "# Разбить таблицу данных на матрицы X и y\n",
    "X = adult[list(set(adult.columns) - set(['salary']))].values\n",
    "y = adult['salary'].values\n",
    "\n",
    "# Добавить фиктивный столбец единиц (bias линейной модели)\n",
    "X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = LogisticRegression()\n",
    "\n",
    "LR.fit(X,y)\n",
    "\n",
    "y_pred = LR.predict(X)\n",
    "\n",
    "round(f1_score(y,y_pred),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3B.6.2\n",
    "Посчитайте confusion matrix для классификатора из задачи 3.6.1. Введите значения получившейся матрицы в соответствующие ячейки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23028,  1692],\n",
       "       [ 3125,  4716]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23028,  1692],\n",
       "       [ 3125,  4716]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP = ((y==y_pred) & (y == 0)).sum()\n",
    "TN = ((y==y_pred) & (y == 1)).sum()\n",
    "FP = ((y!=y_pred) & (y == 1)).sum()\n",
    "FN = ((y!=y_pred) & (y == 0)).sum()\n",
    "\n",
    "np.array([[TP,FN],[FP,TN]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3B.6.3\n",
    "Постройте ROC-кривую и посчитайте  для классификатора из задачи 3.6.1.\n",
    "Ответ округлите до сотых. Пример ввода: 5.55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = LR.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0xa960a20>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH3CAYAAABTi52XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3Xl8VOW9x/HPL/tCSICwhi2yVBABERdcKrgiWveqtGpbbW29VWtr92r12nq7t7Z1qdpa622LiteVYtVWtMW6gKKiLMpOwhIgG2TPzHP/OJMYhkkywGTOzOT7fr3mlcw5Z878JvDKN89znvM85pxDREREUlOa3wWIiIhIz1HQi4iIpDAFvYiISApT0IuIiKQwBb2IiEgKU9CLiIikMAW9SIyY2a1m5jo8tpnZAjOb3Mnxh5nZI2ZWYWaNZvaBmd1mZvmdHD81dPw2M2s2sy1m9qCZTYyitnFm9pyZ1ZrZdjObb2aDkv1zdTjHn0K1XRVh38zQvkkR9p0d2jc6bPsYM/uDmW0O1bTDzB4zs2OjrUkkUSjoRWKrBpgRetwAjAdeMLP+HQ8ys1nAEmAEcB1wBnAv8GXgJTPrE3b8BcAbwADgq8CpwNeBYuCVrgoyswzgaWAgMBe4HsgGhibz5+pwjhzgvNDTufvxmTo73/HAW8AU4Puhmr4ENAGvmFnhwb6HSFw55/TQQ48YPIBbgZ1h244FHPCpDtvygC3Av4HMsOMnA83AHR22DQN2A38CLML7nt1NXYeFavh4Kn2uDsddGKrlH0AAGBK2f2Zo/6RI7xHaNzr0PBcoC50rK8Lxs4A8v/+v6aHH/jzUohfpWe+Evo7osO2TeK3p7znnWjoe7Jx7F/gL8Hkzywtt/jyQBdzonNtnKkvn3IJuagiEvo7dz9q7kgifq81coBy4Fq+X8uJoP0QEnwRKgK8655oj1LTIOVd/EOcXiTsFvUjPGhn6ur7Dto8DVc65f3XymieBfGBa6PlJwFLn3M4DrGE18CbwIzMbdYDnCJcInwszKwDOAh51zq3C63I/mO77k4AtzrnlB3EOkYSioBeJMTPLCD3GAHcCbwNPdTikBNjYxSk2djiu7eumgyjpUKA/UA88G35dPVoJ+LkAzgdygIdDzx8GjjWz0gM8XyxqEkkoCnqR2BoAtIQea4AjgAucc00Hed4DWn0q1E2+EHgAOA7vGvQzZpYb2n9UZyPSw/j2ucwsrcMfGRlm1vH31lxgnXPujdDzh0PnvLQnaxJJJgp6kdiqAY7CG6z2Rbxr0H8NC6dyoKsu9FEdjmv7OrKTY7tzHt519Ducc1uB0/Gu1c8zs3S8wN4CrOjmPH5+ru/z0R8ZLaHnmFkx3oj4Z8ysyMyK8Ab3LQE+1eH1raGv6RHOnR52zMH8rEUSkoJeJLZanXNLnXOvO+fuw7ut7Fi8QV5t/gX0M7MTOjnHOUAd3nV1gJeA6QfY5T4KL8QaAJxzHwJnAicD9+Ddavdb51ywm/P4+bnuw/sjo+1xX2j7J4EM4CtAVYfH0cCkDr0UO0Jfh0Q491AgCOzqUFOJmR3WTU0iSUNBL9Kz/gy8D3yrw7b5wFbg9tA97u1C4XQ5cL9zriG0+Q94LdmfR3oDMzuri/dfiXfP/DltG5xzb+Fd274K7371X+3H52kTt8/lnNsS+iOj7bEldMjc0OebFfaYHTpvW/f9h8A24NwIb3Mu3oDAtpoew2vV/8rMMiPUNLPDXQMiycHv+/v00CNVHkS43zy0/VN4131P6bBtFt7guMV4t4OdBHwN2InX9dwn7BwX4t2H/jxegJ0Y+voEUNlFTemh99gN3AScAnwaeBkv/Frxbm9Lts81HK8l/q1O9j8NrO3w/Euh4+8G5uAF/CN4tx7OCXvt8UAt3kQ+nwnVdD7e/f6tQKHf/9f00GN/Hr4XoIceqfLoIhDTgQ+A58K2TwIexetabgodcxuQ38n5jwgdvx2vxboFr2U9rZu6+uC1mjeFXrcRrxVfjNciDwKXJNPnwps9LwAM72T/xaE/Qo7psO0yvMsGjXiXEBYDZ3by+rF4AxjLQjXtCP3xMcvv/2d66LG/D3NOA0xFRERSla7Ri4iIpDAFvYiISApT0IuIiKQwBb2IiEgKU9CLiIiksIzuD0l8xcXFbvTo0X6XISIiEhdvvvnmTufcwGiOTYmgHz16NEuXLvW7DBERkbgws65WityLuu5FRERSmIJeREQkhSnoRUREUpiCXkREJIUp6EVERFJYSoy6j0ZtbS0VFRW0tLT4XYoksMzMTAYNGkTfvn39LkVEJCZ6RdDX1tayfft2SkpKyM3Nxcz8LkkSkHOOhoYGysvLART2IpISekXXfUVFBSUlJeTl5SnkpVNmRl5eHiUlJVRUVPhdjohITPSKoG9paSE3N9fvMiRJ5Obm6hKPiKSMXhH0gFryEjX9XxGRVNJrgl5ERKQ3UtCLiIikMAV9knHOUVpaipmxZs2affbfeuutFBcXR3zt17/+dSKt8vfSSy9x9tlnU1xcTFZWFqNHj+b6669n06ZNsS4/oqeeeorDDz+cnJwcJk6cyCOPPBLV65588kkmT55MdnY2paWl/PKXv9znmLvvvpuzzjqLAQMGYGa89NJLMa5eRCSxKeiTzKuvvsqGDRsAePjhhw/6fL/5zW84+eSTyc3N5d577+Uf//gHt9xyC8uWLePcc8896PN3Z/HixVx44YXMmjWLZ599lrPOOou5c+fy/PPPd/m6V155hQsuuICjjz6aZ555hiuvvJJvfetb3HHHHXsd99BDD1FZWckZZ5zRkx9DRCRxOefi9gAeACqA9zrZb8BvgDXAu8C0aM575JFHuq6sWLGiy/3J5Nprr3X5+fnumGOOcRMnTtxn/y233OIGDBgQ8bU33nijGzVqVPvzt956y6Wnp7ubb7454vHPPPNMTGruyumnn+5mzZq117YzzzzTHX/88d2+7sQTT9xr21e/+lXXr18/19TU1L4tEAg455xbvny5A9yiRYuiqiuV/s+ISOoBlrooszfeLfoHgdld7D8TGBd6XA3cE4eakkYgEGD+/Pmcc845XHnllaxYsYJ33333gM/329/+luLiYm6++eaI+88+++wDPnc0mpqaWLRoERdffPFe2y+99FJeffVVampqOn3t22+/zamnnrrXttNPP52qqipeffXV9m1paeq0EpHeLa4z4znn/mVmo7s45FzgodBfK6+ZWZGZDXXObY1LgQnuxRdfZPv27Vx66aWccMIJXHvttcybN4/Jkycf0PlefvllTjnlFDIzMw/o9a2trd0ek56e3untamvXrqWlpYVDDz10r+0TJkwgGAzywQcfcNRRR0V8bWNjI1lZWXtty87OBmDlypWcdNJJ0XwEEUligWB7bzCuw3bn9j7Oddgbvm+v4zrsc7gu9nXcHnZcJ68xg745B/a79mAl2hS4JcDmDs/LQttiHvT//cz7rNhSG+vTRmXisL7c8onD9vt18+bNo6ioiNmzZ5OVlcVpp53Gww8/zP/8z/8c0L3f5eXljBw5cr9f1yaaPxD++Mc/8tnPfjbivqqqKgCKior22t6vX7+99kcyduxYlixZste2N954A4DKyspu6xJJRI0tAXY3ttLYEmBTZT3OQdB5keN1w3oBFAx6gRIMbQNH0LHP8VtrGsnLSicQdO2P1tDXsqp6cjPTceBtC3j7gs6xYVcdfbI/iof284be34Xeq+37j94zVGfYa9bs2MOgguz2c0GE8+31GT86TzAYily392duaAnE5x8lRvrmZPDurf6MFUq0oI+UVhH//jKzq/G69w8qrJJFU1MTTzzxBOeff357S3bu3LlcfvnlvPbaa8yYMeOAznswk8OEB20kpaWl+11D21/IXdX2pS99iWuuuYb777+fiy66iDfeeINf/OIXgNeLILK/nHO0BBzNgSAtrUFqG1toCQQJBKE1GCTY9tU5qupaCLiPgrM1EKSxJUh5dT15WRm0BIK0BIKsqdhDYW5me7i2BeoHFbspzM1k/c46AkFHMOioa/YnuPrlZZKelkZGmpHe4bF6227GDy7AzPvFnGYW+t77mpYGRlroa2ifGYbXek3r8P2wohx21TVTOiAfwvbt9drQe7VtT7OPfg+En9PM2LmnyTtnSMdfGV39/gjfZR2iZ69zdPG6zl6z73t5O7My/LuMmGhBXwaM6PB8OLAl0oHOufuA+wCmT5/eRWdMZAfSovbTs88+S3V1NXPmzKG6uhqAmTNnkp2dzbx589qDPiMjg0Ag8i+MQCBARsZH/+QlJSUHdQvd1KlTuz2mq9Bta7m3fZ42bc/DW/odXXnllbzzzjtcc801XH311eTl5fGTn/yE6667jsGDB0dTvqQg5xwNLQE27KxnxdZaDPYK3d1NrdQ1tRJ0sLWmgd2NrTS1BNlW2xjzWjLSvOBqCTiG98ttD9LM9DRyMtKpqG3i6NH9qW1sYcLQvuRkei3vfnlZDOiThXOOEf3zyEpP2ytE2wI3LTwA9wrCj0IzLyuDnMw00tLsozA376tmgewdEi3onwauNbOHgWOAGl2f98ybNw+AT37yk/vse/TRR/nVr35Feno6AwcOpLa2lvr6evLy8vY6buvWrQwaNKj9+cyZM1m4cCGtra17/QEQrYPtuh8zZgyZmZmsWrVqr2vqq1atIi0tjfHjx3d63vT0dO68805+8IMfUFZWRmlpKatWrQLg2GOP3b8PIgklEHRU1jXTEgjSGmphV9c3hwK8DjOjJRCkpqGF1dt2s6eplaBz/Gftri6vv3Y0blAf8rMzOKQ4n4EF2WRnpFPT0MK4QX3IzkwjMz2N1oCjMC+TPtkZe4Vj28OA/n2yyEgzMtLSyEg3sjPS6ZubQWaaF6wiiSCuQW9m84CZQLGZlQG3AJkAzrnfAQuBOXi319UDn4tnfYlqz549LFiwgLlz53L11VfvtW/ZsmV87WtfY9GiRZx66qmceOKJBINBFixYsNdo9rq6Ov75z39y5ZVXtm+77rrr+NOf/sTtt9/OLbfcss/7Lly4kDlz5nRa18F23WdnZzNr1izmz5/PF7/4xfbtjzzyCDNmzKCwsLDb8/fr16+9Z+Duu+/muOOO22dwn8RfMOhYt3MPFbVNrNmxh5aA173dGnS0BIKs31lHfnYGjS0BVmyppSgvk1XbdtPYEqCxJbjf73fokAKOH1NMazDI0aUDGN4vl5H98xjSN4esjDSyMtLIyUzf67qzSG8R71H3c7vZ74Avx6mcpPHUU09RX1/PV77yFY455pi99h1//PHcfvvtzJs3j1NPPZWJEydyySWXcNVVV7F+/XqOPPJIKioq+MUvfoFzjuuvv779tVOnTuWXv/wlN9xwAytWrODSSy+luLiY9evX88ADD1BTU9Nl0E+fPv2gP9vNN9/MzJkzueGGGzjvvPNYuHAhCxcu5O9//3v7MRs3bmTMmDE88MADXHHFFQC89tprLF68mKlTp1JbW8u8efN47rnnWLx48V7nX7p0KRs2bGDzZm+M58svv8zOnTsZPXp0TOrvjZxzbKttZP2OuvYQr9jdyFsbq9i1p5naxlZ27mmK6lwlRblkZ6axbkcdk4YVMqhvNiP759HYEmT0gDwy0tPITPdazDmZaQzum8OAPlnkZqaTlZFGdkY66Wo5i3RJf94mgXnz5jFu3Lh9Qh687vOLL76YefPmcffdd5Odnc1DDz3ED3/4Q+677z42bdpEQUEBM2fO5C9/+QslJSV7vf7666/n8MMP5+c//zmf//znqa2tpaSkhDPOOINvfOMbPf7ZTjjhBB577DFuuukm7rnnHkpLS/nrX//K6aef3n6Mc45AIEAw+FFLLzMzk0ceeYRbb72VtLQ0TjzxRF555RUOP/zwvc5/55138qc//an9+a233grAZz7zGR588MEe/WzJLBB0bK6sZ+eeJsqrG6hrCrB0YyXLNlWzfmddp6/rk53B+MF9+MSUoWRnpDN5eCEj+uUxuDCb/KwMMtJN3doicWbh9wAmo+nTp7ulS5d2un/lypVMmDAhjhVJsust/2fqm1vZVFnP9tom3tlcTcXuRl5fV0l5dQP1YaPAzaC4TzZThhcyfnABR47qx8CCbAYV5NA/P4vMdA3uEokXM3vTORdVt6Ra9CK9yJ6mVt5Yv4s/vrKB/6zdRSC47x/6hwzMZ/LwQk6dMJgBfbIY2T+PwtxMhvfLIydTty6KJBsFvUiKCgYdb26q4o5/fMB75bXUN7fSEvgo2If0zWFSSSFnTR5CUV4WA/KzOLykUK1ykRSjoBdJERt21vHGhkrKqxp4Y30lSzZU0tqhxX7KoYOYMqKI/vlZnD15KEV5WV2cTURShYJeJElV1TXz1zc2UdPQwsurd7B6++699g8qyOaiI4dz7tQSPjakwKcqRcRvvSbonXPqkpSoJOIA1drGFjburOc/a3eybFM1q7bVsmFXffv+kqJcTj50EBdPH86xhwwgPzuDzHSt3CcivSToMzMzaWho2GemOJFIGhoaDnhFv5jV0Bxg2eYqFq2qYPX2Pfzrgx177c9MN06dMJjZk4Zw5qQh5GsiGBHpRK/47TBo0CDKy8spKSkhNzdXLXuJyDlHQ0MD5eXlvsyXv7yshnlLNvHY0jKaA3vPDnfe1GFMH92fcYP6MHVkEdkZGv0uItHpFUHft29fALZs2UJLS4vP1Ugiy8zMZPDgwe3/Z3pS26j4/311I0+/89HaTSVFuUwb1Y+jRvfjiBH9mDisr2Z/E5ED1iuCHrywj8cvb5GuVNU1s2xzFT977gNWbq1t394/P4tjSvvz5VljmVTS/Rz/IiLR6jVBL+KHzZX1/GHxel5YsZ1ttY17TVBTWpzPuVOHcdbhQxk3WKPiRaRnKOhFYiwQdNzz0hr+763y9nnhi/IyOXJkP4b3y2XGmAEcUzqAkQM0OFREep6CXiRG/v7eVu7/93re31LTvtTqyYcO4qunjufw4eqOFxF/KOhFDkAg6Hj5gwqefnsLW2saeX19Zfu+w4b15eqPH8InJg/TKm0i4jsFvUiU6ppaeXTpZl5fV8nf39/Wvj0jzZg4tC8ThvblprMm0C9fU8uKSOJQ0It048Ptu3l06Wbu//f69m1FeZl8YvIwvn7GxyjM9XdyHRGRrijoRTrxjxXbuW3BCjZVelPNDivM4YrjRvPZ40ZruVYRSRoKepEQ5xzz3yzj8bfKeG3dR9fcjxrdj6tOOITZk4b4WJ2IyIFR0Euv1xoI8ux72/j9v9fxTlkNAIcU53PGpCF84cRD6K9r7iKSxBT00qv95p8f8ssXPgAgPc347pxD+fQxo7RIjIikDP02k15p5dZabnz0HVaEpqH9zIxRfPesCVosRkRSjoJeepWG5gA/fW4Vf3xlAwAnjR/ILy+ewoA+2f4WJiLSQxT00mv864Md3PTke2yqrGf84D7c/ekjGTuoj99liYj0KAW9pLyWQJAfLVzFA69498F/58xDueqEUjLS03yuTESk5ynoJaU9sayM7zy+vH3u+X/eeBJjBqoVLyK9h4JeUtafX9vITU++R25mOj+58HAumDacTLXiRaSXUdBLSgkEHY+/VcYd//iQ8uoGDh1SwPwvzaAgR9PUikjvpKCXlPHmxipueGQZmysbKMjO4LJjR/LN2Ycq5EWkV1PQS9JzzvHMu1u5ft4yAC6ZPoLbz5+kwXYiIijoJcm9vbma8+56pf354/91HNNG9vOxIhGRxKKgl6QUDDpuW7CCB/+zAYDTJg7mpxdO1lrwIiJhFPSSdDbtqucTdy6mpqGFgQXZPPFfxzG8X57fZYmIJCQFvSSN3Y0tfOfx5Sx4dysAn5gyjF9fMpW0NPO5MhGRxKWgl6TgnOPyP7zB25urmVTSl+/OmcBxY4r9LktEJOEp6CXhBYOOS+9/jbc3V3PBESX88pKpfpckIpI0FPSS0Oa9sYlfvfABFbubvAF3F032uyQRkaSioJeE9eAr67n1mRUAXDtrLDeePh4zXY8XEdkfCnpJOLWNLdz85Hs89fYWAB770gymj+7vc1UiIslJQS8JZU3FHub85t80twaZ9bGB/GbuEZrCVkTkICjoJWE8u3wr1/zlLQD+5/zD+dQxI32uSEQk+SnoxXeBoOP2v63kgVfWA3DHJVM574gSn6sSEUkNCnrxVV1TK+fcuZi1O+oY0T+XBz93NGMG9vG7LBGRlKGgF9+8unYXl//hdVqDjtMnDubey4/UqHoRkRhT0IsvNu2q5zN/fIM0M/77nIl85rjRfpckIpKSFPQSd40tAc69azHNrUEtKysi0sPS/C5AepfGlgAf/+kiqupb+MF5kxTyIiI9TEEvcRMIOj51/2tU7G7izElDuPzYUX6XJCKS8hT0Ejffefxd3tpUzRUzRnHPZUf6XY6ISK+goJe4uGvRGh5dWsawwhxuO3eS3+WIiPQaCnrpcQve3cLPnlsNwPxrjvO5GhGR3kVBLz3qhRXbufavy8jKSOPpa4+npCjX75JERHoV3V4nPWZ5WQ1f+vObFPfJ5ulrj2eYQl5EJO4U9NIj3t5czXl3vQLAA5+drpAXEfGJuu4l5v71wY72kP/9FdOZPLzI54pERHovBb3E1ObKer7w0FIAfnfZNE6dONjnikREejd13UvMNLYEmHv/azgHj31pBtNH9/e7JBGRXk9BLzHznceXU1bVwK8umaKQFxFJEOq6l5j40cKVPLGsnJPGD+T8I4b7XY6IiIQo6OWgba6s575/r8MM7rtCU9uKiCQSBb0clMq6Zs65czHOeSPsszPS/S5JREQ60DV6OWAVtY1cev9rVNW38LvLpnHKBI2wFxFJNAp6OWBzfrOYnXuauPrjhzB70lC/yxERkQjUdS8H5M4XP2TnnibOOnwo350zwe9yRESkEwp62W/vldfw8+c/ID8rnV9cPMXvckREpAsKetkv63fWcc6diwF46trjycnU4DsRkUSmoJeoNbUGuOTeVwk6uOzYkYwdVOB3SSIi0g0NxpOoPfSfjVTsbuJbsw/lmplj/C5HRESioBa9RKW8uoHbF65kYEE2V3/8EL/LERGRKCnopVuBoOOCu71lZ79x+sdITzOfKxIRkWjFPejNbLaZrTazNWb27Qj7R5rZIjNbZmbvmtmceNcoHwkGHT9YsILttU188aRDuPioEX6XJCIi+yGu1+jNLB24CzgNKAOWmNnTzrkVHQ67CXjUOXePmU0EFgKj41mneJxzTLnteXY3tnJMaX++PftQv0sSEZH9FO8W/dHAGufcOudcM/AwcG7YMQ7oG/q+ENgSx/qkgysfXMLuxlYOLylk3heOxUxd9iIiySbeo+5LgM0dnpcBx4QdcyvwvJldB+QDp8anNOlo4646Fq3ewUnjB/LAZ48iTdflRUSSUrxb9JHSwoU9nws86JwbDswB/tfM9qnTzK42s6VmtnTHjh09UGrv1dwa5HtPvAfAreccpsF3IiJJLN5BXwZ0HM01nH275q8CHgVwzr0K5ADF4Sdyzt3nnJvunJs+cODAHiq3d/r586tZvGYnp08cTGlxvt/liIjIQYh30C8BxplZqZllAZcCT4cdswk4BcDMJuAFvZrscVJe3cB9/1rHiP653Hv5kX6XIyIiBymuQe+cawWuBZ4DVuKNrn/fzG4zs3NCh90IfMHM3gHmAZ91zoV370sPufPFNd7XudM0+E5EJAXEfQpc59xCvFvmOm77fofvVwDHx7sugT1NrSx4ZwsD8rOYMqLI73JERCQGNNe9AFBT38L597zC7qZW/nDpdL/LERGRGNEUuALAD/+2gnU76rh21lhOmTDY73JERCRGFPTC6m27eeytMrLS0/j6GR/zuxwREYkhBX0vFww6rnxwCc7Bwq+c4Hc5IiISYwr6Xu7Hf19FeXUDN589kbGDCvwuR0REYkxB34tV1zfzv69upCgvk88dN9rvckREpAco6Hux255ZQUNLgPsun6657EVEUpSCvpeqrm/m8WXlnDV5KEeX9ve7HBER6SEK+l6qbQa8Tx090udKRESkJynoe6HlZTX8fvF6Jg8v5Pix+6wXJCIiKURB38uUVdXzX399k4w0457LtGiNiEiq0xS4vczNT77H5soGfnfZkZQU5fpdjoiI9DC16HuRtzdXs2j1Dq47eSyzJw3xuxwREYkDBX0vcueLH5KeZlwxY7TfpYiISJwo6HuJ59/fxj9WVnD5saMYWJDtdzkiIhInCvpeoL65le88vpyivEy+PGus3+WIiEgcaTBeL/CDBSvYVdfM3Z+epta8iEgvoxZ9invu/W3Me2MzJ44rZs7hQ/0uR0RE4kxBn8JqG1v476ffp7Q4n7s/Pc3vckRExAfquk9hVz24hC01jfzvVUdTkJPpdzkiIuIDtehT1O9eXsuSDVV8YsowThw30O9yRETEJwr6FLRyay13/OMDBhVk88uLp/hdjoiI+EhBn4K+9ug7ADz6xRlkpuufWESkN1MKpJj5Szezcmst1508jtHF+X6XIyIiPlPQp5j5S8vIzkjjqhNK/S5FREQSgII+hTy5rJw3NlRy1Qml5GSm+12OiIgkgP26vc7MSoAhQA5QCax3zjX2RGGyfxqaA/zwbys5pDif604e53c5IiKSILoNejM7BbgCOBUv5AEMcECrmS0DHgP+7Jzb1lOFSuecc9z05Hvs3NPEry6ZQm6WWvMiIuLptOvezC40s/eAZ4BC4NfAHOBoYDJwEnAV8BrweWCDmd1lZlroPM7mv1nG/71VxscGF3DC2GK/yxERkQTSVYv+f4CfAfOcc3WdHLMY+DOAmU0BbgA+C/w4hjVKNxatqgBg/jUzMDOfqxERkUTSVdAf6pxz0Z7IOfcO8DlT0sRVWVU9z72/jUumj6CvprkVEZEwnXbd70/Im9mgA3mdHLzf/3s9QQdXnajb6UREZF8HdXudmY03s3uBDbEpR/bHrj1N/OX1jRw9uj/jBxf4XY6IiCSgLoPezC4wsyfN7E0ze8zMjgpt/5iZ/R+wArgE+FUcapUwD7yynpaA48snj/W7FBERSVBdjbq/Au+2uUnAZuAQ4CUz+zzwNnAycCswyjn3vZ4vVTraXtvIXYvjufL3AAAgAElEQVTWMmVEESeN1+p0IiISWVeD8W4A5gGXO+eCAGb2LeBeYAlwtnNuZ8+XKJHctmCF9/Wcw3yuREREEllXXfdjgT+2hXzIfXiT5dymkPfP2h17+Nu7Wznl0EFMGVHkdzkiIpLAugr6PkBt2La255oBz0d/fX0TAN87a4LPlYiISKLrbgrc6WbWp8PzNLypb48ys72aks65F2NdnOzLOcf8pZs5afxADhnYp/sXiIhIr9Zd0N/ZyfZ7wp47QBOsx8G/PtxJbWMrcw7XTMMiItK9roJe/cIJxjnHVQ8uITsjjbMnD/O7HBERSQKdBr1zbnU8C5HuLdlQRWvQcd4RJeRn79cKwyIi0kt1N2HOyWY238yWmNkzZva5eBUm+/rzaxvJSDNuPnui36WIiEiS6GrCnPOAfwBHAuXACOD3ZvbDONUmHayp2M2Cd7dwyoRBFOZq8RoREYlOVy367+DNjDfOOXeec24q3kx4N5qZ+o3j7Nf/XEPQwddP/5jfpYiISBLpKugPBe53zgU6bLsbyAZG92RRsq+K2kYAxg7SLXUiIhK9roK+AKgJ21Yd+tq3Z8qRSBqaA7y+vpLLjx2FmfldjoiIJBFNmJMEnnlnCwAnjCv2uRIREUk2mjAnCbz84Q765mRw+sTBfpciIiJJRhPmJLjy6gaef38bZ08epm57ERHZb10F/RTgBedcVbyKkX399O+raAk4rj15rN+liIhIEupqMN48YFy8CpF91Ta2sODdrcw+bAhjtICNiIgcgK6CXv3EPrvjhQ8JBB2fO36036WIiEiS6nIKXPHXgne3UJSXyTGHDPC7FBERSVLdjbq/zMxmRnEe55z7WQzqkZBlm6qo2N3ENTPH+F2KiIgkse6C/iogGMV5HKCgj6Gn3vbunb/s2FE+VyIiIsmsu6Cf5Zx7Iy6VSDvnHPOXbmZE/1yGFeb4XY6IiCQxXaNPQH9bvpW65gDXzhqre+dFROSgKOgT0J/+swGA848Y7m8hIiKS9LoK+u1Ac7wKkY/saQpQmJtJVob+DhMRkYPT6TV659zQeBYinrKqelZureXaWZoJT0REDl6nTUYz+5uZnRDticysyMy+a2bXxKa03unnz60G4Nypw3yuREREUkFXo+7/AzxlZpXAY6Hn7wE7gSagCCgFjgTOBE4D/g18pScLTmXrduzhybe3MG1kEeMGF/hdjoiIpICuuu5vN7PfAp8BrgC+GeEwAyqBJ4CZzrnXeqTKXuLJZeUAfOXU8T5XIiIiqaLL++idc7XAb4HfmlkhMA0YAuTgBfxq59yqHq+yl9ha0wjADE15KyIiMdLdhDntnHM1wKIerKVXa2oNMP/NMk4cV6zR9iIiEjNKlATx+Ftet/0F00p8rkRERFKJgj5B/OTvqxhUkM15UxX0IiISOwr6BLBqWy3V9S2cNnGwprwVEZGYUtAngLaV6q46odTnSkREJNXEPejNbLaZrTazNWb27U6OudjMVpjZ+2b213jXGG+LP9xJYW4mpcX5fpciIiIpJuqgN7P+ZvbfoRnz3jWzCaHt15jZ9CjPkQ7chTfBzkRgrplNDDtmHPAd4Hjn3GHADdHWmIyq6ppZXl7DJUeNULe9iIjEXFRBb2bTgDXA54Bq4DAgN7T7EOAbUb7f0cAa59w651wz8DBwbtgxXwDucs5VATjnKqI8d1J67M0yAGZPGuJzJSIikoqibdHfAbwKjMWbKa9j0/NV4Ngoz1MCbO7wvCy0raPxwHgze8XMXjOz2VGeOym9um4XQwtzmDayn9+liIhICop2wpzpwPnOueZQ93tHO4HBUZ4nUt+0i1DTOGAmMBz4t5lNcs5V73Uis6uBqwFGjhwZ5dsnltZAkBdXVXDJ9BF+lyIiIikq2hb9bqB/J/tKgR1RnqcM6Jhqw4EtEY55yjnX4pxbD6zGC/69OOfuc85Nd85NHzhwYJRvn1iWl9cAMH6IFrAREZGeEW3QLwBuNbOOIe3MrAj4GvBklOdZAowzs1IzywIuBZ4OO+ZJYBaAmRXjdeWvi/L8SeXZ97YBcPbkoT5XIiIiqSraoP8W0AKsAl4Ibfs1Xmsb4OZoTuKcawWuBZ4DVgKPOufeN7PbzOyc0GHPAbvMbAXe3PrfcM7tirLOpLJkQyWZ6cbgvjl+lyIiIikqqmv0zrmdoVvorgJOARbjrV73Q+D3zrmGaN/QObcQWBi27fsdvnd4vQRfi/acyWh7bSPLNlVz1uFqzYuISM/Zn9XrGvHugb+r58rpPV5d63VSXD5jlM+ViIhIKov2Pvr6zibFMbMjzKw+tmWlvj/+ZwO5mem6rU5ERHpUtNfoc7o4NhsIv+VOulBT38K7ZdVMH91Pa8+LiEiP6rTr3syG4d3+1mZihClac/Bmy9sY+9JS17LNVTgHnz/xEL9LERGRFNfVNfovALfgTWjjgD9EOMaAZuCLsS8tdS3ZUAnA+MF9fK5ERERSXVdBfx/e/fMGvIHXcn8v7JhmYL1zbk/PlJd6nHM8/lY5Mw4ZwNDC3O5fICIichA6DXrn3FZgK0BopboNzrmmeBWWqirrmtla06jR9iIiEhfR3ke/GsC8i/RD8a7Nhx+TkrPXxdojS701fSaXFPlciYiI9AZRBb2ZZQA/A64EOruwrJH33WhoDnDHCx8yuG82x40Z4Hc5IiLSC0R7b9d3gUuAG/Cu2X8N+C/gFWADcGFPFJdq7n5pDc2BID+7aAppaZEW8hMREYmtaIP+U8CtwEOh54udc/c65z4OvA6c1gO1pZylG6oYVpjDx8cn52p7IiKSfKIN+pHASudcAGgCOl5g/hNwcawLSzXOOT6s2M1xY4v9LkVERHqRaIN+G1AY+n4DcHyHfaP24zy91toddezc08zk4YXdHywiIhIj0S5q8y+8cF8APADcbmaj8Vr3lwGP90RxqWR5eTUAk0oU9CIiEj/RBv1NwKDQ9z8Pve4iIBcv+G+KfWmp5e1N1ZjBpGEKehERiZ9ugz50a91gYDO0rxf/o9BDovTGhiqOHKlFbEREJL6iSZ0g8CowuYdrSVl1Ta2s3FrLtFFaklZEROKr26B3zgWBNYCGix+gd8q86/NjB2oRGxERia9o+5FvAb5vZuN7sphUtWyTF/QzNBueiIjEWbSD8a4HBgArzGwdsB1v6dp2oclzJIKlGyoZ3i+X4f20Wp2IiMRXtEFfFnrIfgoGHe9tqeXo0v54awKJiIjET7Sr183t6UJS1Strd7JjdxPHj9EQBxERiT/d69XDnnt/GxlpxiemDPW7FBER6YUU9D1s0aodTBlRREFOpt+liIhIL6Sg70EtgSA79jRRUqRBeCIi4g8FfQ9a8O4WmluDnDVZ3fYiIuIPBX0Puv9f68lKT+MkrT8vIiI+ifb2OgDMbAwwDRgB/Nk5V2FmI4Bdzrn6nigwWTnnqNjdxOjiPHIy0/0uR0REeqmogt7McoF7gbmAhR4vARXAHcBa4Js9U2Jyeq+8lp17mvjyrDF+lyIiIr1YtF33vwBOA84BCvGCvs3fgDNjXFfSe/qdcgCOPUTT3oqIiH+i7br/JHCjc+5ZMwvvh14PjIptWclveXkNABOG9vW5EhER6c2ibdHn481v39m+YGzKSR3LNlUzZUSR32WIiEgvF23Qvwl8qpN9FwCvx6ac1FDb2EJTa5Dxg7QsrYiI+CvarvvvA8+Z2QBgPt7Kdaea2TV4fwDM6qH6ktK/PtgBwBEj+/lciYiI9HZRteidc4uA2cAg4AG8wXg/xrvVbo5z7tUeqzAJrdq6G4AzDhvscyUiItLbRX0fvXPuReBoMyvEW5u+yjlX1WOVJbEnlpXzscEFDOiT7XcpIiLSy0XVojezuWaWD+Ccq3HOrVPIR1bb2EJ5dQPjhxT4XYqIiEjUg/EeAirM7FEzu8DM1FTtxNqKPQDMmTTE50pERESiD/phwDfwrtE/Cuwwsz+b2VlmpvVXO1i02huId9iwQp8rERERiX4w3g7n3N3OuZl489zfDJQCTwPbzez3PVdicnlzYyU5mWmM6K+laUVExH/7vXqdc26rc+7XzrnjgbOBBuBzMa8sSb2yZhdnTx6GmXV/sIiISA/br9XrAMxsHHBJ6DERKAN+GeO6klJNfQsARbm6miEiIokh2tXrSoGL8cJ9Ct50uI8BX3TO/afnyksu723x5rc/urS/z5WIiIh4om3RrwV2Af8H3Ai85JxzPVZVknovtJDN1JGa415ERBJDtEE/B3jBORfoyWKS3csf7GBQQTaDCnL8LkVERASIftT93xXy3Vu6oYr++Vl+lyEiItKu0xa9mT0E3OKcWx/6vivOOfeZ2JaWXBqaAzQHgowakOd3KSIiIu266rofB7T1QY/HW7FOOvH25moALjpyhM+ViIiIfKTToHfOzejw/bHxKSd5rdhaC8DEYX19rkREROQj0S5q800zizh5u5kNNrNvxras5LO8rJpBBdkMK9RAPBERSRzRzoz3I2BkJ/uGh/b3apurGigtzteMeCIiklCiDXqj82v0w4Dq2JSTnJxzLC+v4ZCBffwuRUREZC9djbr/NPDp0FMH3GFmNWGH5QDTgJd6pLoksWN3E82tQYb300I2IiKSWLoadR8E2u6dt7DnbaqAu4Bfx7605PH0O1sAOEIz4omISILpatT9PGAegJnNA25yzq2NV2HJ5MVVFYzon8txY4r9LkVERGQv0c6MN1chH5lzjv+s3cWkYYV+lyIiIrKPrq7R3wb8zjm3JfR9V5xz7pbYlpYcdu5pBmBYka7Pi4hI4unqGv0XgMeBLaHvu+KAXhn0b22qAmD6qH4+VyIiIrKvrq7RD430veytbWnaE8bp+ryIiCSeaO+jl068ubGK4f1yKcjJ9LsUERGRfUQ7Be4nzOyKDs9HmtmLZrbNzP5iZgU9V2Ji21LdwAAtTSsiIgkq2hb9LcDADs9/C4wGfgecCNwe27KSQ01DCxt21XP8WHXbi4hIYupqMF5HY4F3AcysLzAb+KRz7mkzW4MX9Nf3TImJ6+UPdgAwRlPfiohIgoq2RZ/GR7PifRxvlP0LoeebgEExrisprAotTXv6YYN9rkRERCSyaIN+OXCJmWUCVwIvO+caQvuGAzt6orhEt2RDJYeXFGognoiIJKxog/57wFygATgd6DiBzrnAGzGuK+HtaWpl2aZqpo7Q/PYiIpK4orpG75x7ycxGAxOAD5xzHVvwDwMfxL60xLa5sp7WoOOo0v5+lyIiItKpaAfj4ZyrBF6JsP2JmFaUJHbsbgJgSN8cnysRERHpXNQT5pjZdDN7wsx2mFlr6OvjZjatJwtMVKu2eQPxivvoHnoREUlcUbXozWwm8BywEbgf2A4MBi4E/mNmZzjnXu6pIhNRTUMLoMVsREQksUXbov8x8HfgUOfcd51zv3bOfRfvmv3zof1RMbPZZrbazNaY2be7OO4iM3NmNj3ac8fTkg1VDOmbQ05mut+liIiIdCraoJ8M3OOcC3bcGHp+NzAlmpOYWTpwF3AmMBGYa2YTIxxXgDcBz+tR1hd35VUNZGaY32WIiIh0Kdqg3w2M6mTfqND+aBwNrHHOrXPONeON2D83wnE/AH4KNEZ53rgrr25gVP98v8sQERHpUrRB/3/Aj0Pd6engtc7N7CLgR8BjUZ6nBNjc4XlZaFs7MzsCGOGcWxDlOeNu157QiPtCjbgXEZHEFu3tdd/AG3z3KNBqZlVAv9DrHw/tj0akvm7XvtMsDfgV8NluT2R2NXA1wMiRI6N8+9hYHlqD/thDBsT1fUVERPZXtBPm1AEXmtlUvO73IcBW4A3n3Dv78X5lwIgOz4cDWzo8LwAmAS+ZGaH3edrMznHOLQ2r6T7gPoDp06c74qi2sRWAcYO0mI2IiCS2qCfMAXDOvQ28fRDvtwQYZ2alQDlwKfCpDuevAdrXfDWzl4Cvh4e83ypqvaEDw/vp1joREUlsUQd96Nr8p/Fa9EPxWvSvA/Occ63RnMM512pm1+Ldk58OPOCce9/MbgOWOuee3t8P4Ie2e+iL8jRZjoiIJLZoJ8wZBzwLjAbeAyqAMcCXgFvMbI5zLqr57p1zC4GFYdu+38mxM6M5Z7yt31lHcZ9s0tN0e52IiCS2aEfd3w80402YM9U5d7pzbirehDnNwL09VWAi2lzVQJ9sTZQjIiKJL9qgPwb4nnNuTceNzrkPgZuBY2NdWKIrzNUa9CIikviiDfpNdN7Nn4E3mr7XeGdzNWMGasS9iIgkvmiD/nvAD0O317ULTW5zG/DdWBeWqOqbvXGHBTn7dcOCiIiIL6JNq68AhcCbZrYJbzDeIGBk6PvrzOy6toOdcx+PdaGJYuVWb7bfaaP6+VyJiIhI96IN+jL27Z5fB7wW23IS39qKPQAU98n2uRIREZHuRTsz3tyeLiRZNAW8Bfx0jV5ERJJBtNfoJeTtTdWYwYA+mixHREQSn4J+P72/pYas9DQy0/WjExGRxKe02k8tgSClxVqHXkREkoOCfj8451i7o07L04qISNJQ0O+HsqoGAIryNCueiIgkh/2a9cXMxgDT8NaU/7NzrsLMRgC7nHP1PVFgItmwqw6AcYMKfK5EREQkOtGuXpeLt3DNXMBCj5fwJsu5A1gLfLNnSkwcb26sAmDaqCKfKxEREYlOtF33vwBOA87BmyGv4/qsfwPOjHFdCSkYdAAMKsjxuRIREZHoRNt1/0ngRufcs2YWvj7remBUbMtKTK+tqyQz3bQOvYiIJI1oW/T5wPYu9gVjU05iM4OWgPO7DBERkahFG/RvAp/qZN8FwOuxKSexrdxaywzdWiciIkkk2q777wPPmdkAYD7ggFPN7Bq8PwBm9VB9CcM5R21jK320PK2IiCSRqFr0zrlFwGy8pWkfwBuM92O8W+3mOOde7bEKE8SO3U0ATBja1+dKREREohd189Q59yJwtJkVAgOAKudcVY9VlmBWb/fWoT9ipG6tExGR5LHf/dDOuRqgpgdqSWg793gt+v55WrVORESSR7QT5jzU3THOuSsOvpzE1Tbavn++gl5ERJJHtC36cRG29QcOAXbi3Uuf0tbt8Ka/zc/WYDwREUkeUaWWc25GpO2hue/nA7fFsqhElJXhjVssytWCNiIikjwOavU659xa4EfAz2NTTuIqr2qgb04GaZoVT0REkkgslqltohdMgftuWTXFBdl+lyEiIrJfoh2Md0iEzVnABLwW/VuxLCrRBIOO7bWNTBvVz+9SRERE9ku0I8vW4M2GF86A5cDVMasoAe1pbqW2sZXjxmj6WxERSS7RBn2kZWgbgbLQdfqUtjM0K152RvjCfSIiIomt26A3s2xgEvC8c255z5eUeLbVNAJQWpzvcyUiIiL7p9vBeM65Jrzb5/r3fDmJqaq+BYBBfTUYT0REksv+LFM7pScLSWQVu70WvWbFExGRZBPtNfqvAA+bWT2wENhO2OA851wwxrUljHc2V5OVnkZxvlr0IiKSXKIN+jdDX+/t4piUHanW0BIgPc00WY6IiCSdaIP+v4h8e12vsHFXPdNH6x56ERFJPp0GvZl9HHjLObfHOfe7ONaUcLbVNnKkJssREZEk1NVgvEXAxHgVkqiaW4NU17dQ3EfX50VEJPl0FfS6IA3s2ONNljOkMMfnSkRERPZfLBa1SWnV9c2AlqcVEZHk1N1gvDlmdmg0J3LOPRSDehJOVZ03WY7uoRcRkWTUXdB/P8rzOCAlg35Xndd1X5inFr2IiCSf7oJ+FrA0HoUkqvfKawAYXKBr9CIikny6C/oG51xdXCpJULUNrQD0U9e9iIgkIQ3G60bF7kbGDerjdxkiIiIHREHfjY276knX1LciIpKkOu26d87pj4AQjbgXEZFkpTDvxrqddXxsSIHfZYiIiBwQBX0XGlsCAASDvXY9HxERSXIK+i7s2O3dQ/+xIX19rkREROTAKOi7UF7dAMCgAi1oIyIiyUlB34Xm1iAAWRn6MYmISHJSgnWhss5b0GZAH426FxGR5KSg78KeJm9WvNzMdJ8rEREROTAK+i60jbrXffQiIpKsFPRdKK9uwAz65mjlOhERSU4K+i5srmwgMz2NNE2BKyIiSUpB34WyqnrdWiciIklNQd+FnMx00kyteRERSV4K+i6UVdVzqOa5FxGRJKag70J9c4CG0Mh7ERGRZKSg74RzjvrmAMP75fldioiIyAFT0HeiJeCtWFeYq1vrREQkeSnoO9HU6nXZD9BkOSIiksQU9J3Y3ehNf9uqtehFRCSJKeg70dA+/a267kVEJHkp6DvRNs99Ya667kVEJHkp6DtRXd8CQF6WVq4TEZHkpaDvRENzW4teXfciIpK8FPSd2FrTAHjT4IqIiCSruAe9mc02s9VmtsbMvh1h/9fMbIWZvWtm/zSzUfGuESA9zfvRqOteRESSWVyD3szSgbuAM4GJwFwzmxh22DJgunNuMvAY8NN41timOXQffZ/sDD/eXkREJCbi3aI/GljjnFvnnGsGHgbO7XiAc26Rc64+9PQ1YHicawRgY6VXQlaGrm6IiEjyineKlQCbOzwvC23rzFXAsz1aUSd27WkG1HUvIiLJLd790pEWd4849ZyZXQZMB07qZP/VwNUAI0eOjFV97TLT00hPM0zr0YuISBKLd4u+DBjR4flwYEv4QWZ2KvA94BznXFOkEznn7nPOTXfOTR84cGDMC21sCVBanB/z84qIiMRTvIN+CTDOzErNLAu4FHi64wFmdgRwL17IV8S5vna1jS3kayCeiIgkubgGvXOuFbgWeA5YCTzqnHvfzG4zs3NCh/0M6APMN7O3zezpTk7Xo2obW+mbo6AXEZHkFvckc84tBBaGbft+h+9PjXdNkdQ2tFBSlON3GSIiIgdF9451oqkloHvoRUQk6SnoO9EccGSk68cjIiLJTUnWiebWAFkKehERSXJKsk7UNrb6XYKIiMhBU9BH0BIIAqC5ckREJNkp6COobWgBoLhPts+ViIiIHBwFfQT1zd7KdQW6j15ERJKcgj6Cumbv+vyAfLXoRUQkuSnoI6gMrVzXLz/T50pEREQOjoI+grLqBgByM7VErYiIJDcFfQTpoeH2/fOzfK5ERETk4CjoIyirCrXos9SiFxGR5Kagj6Dt/vmCbF2jFxGR5Kagj6C5NUiaQU6mfjwiIpLclGQRrNu5h4z0NExT44mISJJT0EfQJzuD5tag32WIiIgcNAV9BLUNrYwZmO93GSIiIgdNQR9BdUMz/fJ0a52IiCQ/BX0Eza1BcjRZjoiIpAAFfQTrd9aRma6BeCIikvwU9BH0z8+isq7Z7zJEREQOmoI+gpaAo7RYg/FERCT5KegjaAkEycrQj0ZERJKf0iyCrTWNZKbrRyMiIslPaRZBVnqartGLiEhKUNBHYjByQJ7fVYiIiBw0BX0ErYEgmWn60YiISPJTmoUJBh1BBxm6j15ERFKAgj5MS9BbzEaD8UREJBUozcI0hVatq2lo8bkSERGRg6egD9MacAAMK8zxuRIREZGDp6AP0xrqus9Q172IiKQApVmYQNBr0WekaTCeiIgkPwV9mLau+3QFvYiIpAAFfZjWUIteo+5FRCQVKM3C1DW1ArAn9FVERCSZKejDpJnXZV/cJ9vnSkRERA6egj5M0Hld97pELyIiqUBBH6Yt6DUYT0REUoGCPkxoLF57F76IiEgyU9CHabuPXjkvIiKpQEEfxqnrXkREUoiCPoy67kVEJJUo6MOo615ERFKJgj5Me9e9kl5ERFKAgj5M2zr0abpGLyIiKUBBHyYnMx2Altagz5WIiIgcPAV9GIfXdZ+XneFzJSIiIgdPQR8mdIkeddyLiEgqUNCHaQ96Jb2IiKQABX2YUM5jatOLiEgKUNCHabu9Ti16ERFJBQr6MK77Q0RERJKGgj6MrtGLiEgqUdDvI9R1r2v0IiKSAhT0YdSiFxGRVKKg74SCXkREUoGCPowG44mISCpR0If5aGY8NelFRCT5KejDtM11r657ERFJBQr6MJrrXkREUomCPkz7FLhKehERSQEK+jBtU+CqTS8iIqlAQd8JtehFRCQVKOjD6Bq9iIikEgV9mI9G3SvqRUQk+SnoO6GYFxGRVKCgD+M0NZ6IiKQQBX0YLWojIiKpREEf5qOb65T0IiKS/OIe9GY228xWm9kaM/t2hP3ZZvZIaP/rZjY6nvW13UevFr2IiKSCuAa9maUDdwFnAhOBuWY2Meywq4Aq59xY4FfAT+JZoy7Ri4hIKol3i/5oYI1zbp1zrhl4GDg37JhzgT+Fvn8MOMXiea+brtGLiEgKiXfQlwCbOzwvC22LeIxzrhWoAQbEpTp0H72IiKSWeAd9pPQM7y2P5hjM7GozW2pmS3fs2BGT4gD652czdUQRWekapygiIskvI87vVwaM6PB8OLClk2PKzCwDKAQqw0/knLsPuA9g+vTpMbu0ftrEwZw2cXCsTiciIuKreDdblwDjzKzUzLKAS4Gnw455GvhM6PuLgBed0zQ2IiIiByKuLXrnXKuZXQs8B6QDDzjn3jez24ClzrmngT8A/2tma/Ba8pfGs0YREZFUEu+ue5xzC4GFYdu+3+H7RuCT8a5LREQkFWnEmYiISApT0IuIiPx/e/cfNUdV33H8/SGRHxEkHCKihpCgoEXaWgVBoaEcEBFPEw9JaaqU0ELLob+OSKn0lEIAj4At1VLxB7SAQoXwoyWJaDEJUAgQIIpyIuVHCAESKEmAJkoSIOTbP+59kslkd59J9snu88x+XufM2d2ZuzvfuTu73507d+fWmBO9mZlZjTnRm5mZ1ZgTvZmZWY050ZuZmdWYE72ZmVmNOdGbmZnVmBO9mZlZjTnRm5mZ1ZgTvZmZWY050ZuZmdWYE72ZmVmNOdGbmZnVmBO9mZlZjSkiuh1D2yStAJ4dwJccBawcwNfrVa7H9rkO2+c6bJ/rsH0DXfxhqqgAAAxASURBVIf7RsQ7qxSsRaIfaJIWRMTB3Y5jqHM9ts912D7XYftch+3rZh266d7MzKzGnOjNzMxqzIm+sSu7HUBNuB7b5zpsn+uwfa7D9nWtDn2O3szMrMZ8RG9mZlZjPZ3oJR0n6QlJiySd02D5TpKm5+UPShrb+SgHtwp1+EVJj0l6VNJcSft2I87BrL86LJSbLCkkufdzA1XqUdKJeX/8haTvdzrGwa7C53mMpLskPZI/08d3I87BStLVkpZLWthkuSRdnuv3UUkf6UhgEdGTEzAMeBrYD9gR+DlwYKnMnwHfzvenANO7HfdgmirW4VHAiHz/DNfh1tdhLrcbcA8wHzi423EPtqnivrg/8AiwR368V7fjHkxTxTq8Ejgj3z8QWNLtuAfTBIwHPgIsbLL8eOBHgIDDgAc7EVcvH9F/DFgUEYsj4g3gRmBiqcxE4Lv5/i3A0ZLUwRgHu37rMCLuiog1+eF8YHSHYxzsquyHABcBXwXWdTK4IaRKPf4JcEVEvAoQEcs7HONgV6UOA3hHvr878EIH4xv0IuIe4JUWRSYC34tkPjBS0ru3d1y9nOjfCzxfeLw0z2tYJiLWA6uAPTsS3dBQpQ6LTiX9mrVN+q1DSb8F7BMRP+hkYENMlX3xAOAASfdJmi/puI5FNzRUqcNpwEmSlgI/BP6yM6HVxtZ+Zw6I4dt7BYNYoyPz8l8QqpTpZZXrR9JJwMHAkds1oqGnZR1K2gH4GnBKpwIaoqrsi8NJzfe/Q2pZulfSQRHxf9s5tqGiSh3+AXBtRFwm6ePAdbkON2z/8GqhKzmll4/olwL7FB6PZstmqI1lJA0nNVW1apbpNVXqEEnHAH8HTIiI1zsU21DRXx3uBhwE3C1pCem83kx3yNtC1c/zjIh4MyKeAZ4gJX5LqtThqcBNABHxALAz6RruVk2l78yB1suJ/mFgf0njJO1I6mw3s1RmJjA1358M3Bm5R4UBFeowNzt/h5TkfU50Sy3rMCJWRcSoiBgbEWNJ/RwmRMSC7oQ7aFX5PN9G6hyKpFGkpvzFHY1ycKtSh88BRwNI+jVSol/R0SiHtpnAybn3/WHAqoh4cXuvtGeb7iNivaS/AO4g9Ta9OiJ+IelCYEFEzAT+jdQ0tYh0JD+lexEPPhXr8B+AXYGbcz/G5yJiQteCHmQq1qH1o2I93gEcK+kx4C3g7Ih4uXtRDy4V6/As4CpJZ5KanE/xwc8mkm4gnRoalfsxnA+8DSAivk3q13A8sAhYA/xRR+Lye2RmZlZfvdx0b2ZmVntO9GZmZjXmRG9mZlZjTvRmZmY15kRvZmZWY070VkuSpuWR3srTnK18nXmSbtxecXaDpKWSLik8niLp5AblhsS2S9o7v99jBvh1vyRpduHxaU32qccLZeYV5q/Po5RdJmm3FmWekfQtSXuW1v+3ku4YyG2y3tSz/6O3nrAKKF/PfFU3AhlkfhdYWXg8hXStg++Vyv0p8EangmrD3qT/K88hXdClbTkxnw38foPFR7J5vawtLZ8D/D3p+/VQ4ELS9cynNClzMGnQonFsvr9+EzhH0hERMW+bN8Z6nhO91dn6PEKUFUTEIxXLPba9Y2lG0jBgWB5FrRtOAn4VEXMbLHsoIlqNIvhyYb+bl380nC/pjL6R8xqU2RW4SNJefVeQjIhVkv6TNHCME71tMzfdW8+SdLakBZJWS3pJ0gxJ7+vnOWMk3SJphaS1uWl2WqnMkZLukbRG0suSvpO/yFu97vV5RLUTJD0haV1+jQ+Wyr1d0jdyvOskPZTHEiiWGZ+bh1fn6RFJJxSWb2y6l3Q9aejMowvNyefmZRub7iV9Mi/7QGlde0p6U9LUwrx2tn9SvnLdOuCjkt4r6ZrcvL1W0pOSLpD0tvy895PGmIc0SE1IWl+K7ypJy3N9zZN0SKtYsqnArRXKVfGTfDu2RZmf59t9SvNvBSZK2n2AYrEe5ERvtSZpeGkqjh41GrgcmEBqpt6JTUdgzVwPvBs4jXQpy4tJ1/vuW994YDawDJgEfDG//r9WCHc/0iWDpwGfIw2J/F+SdiqUuRo4mdQcfALwIvAjpZHEkDQSmAU8mdc/Gfh3YI8m6zwfuId0nfOP5+maBuXuBJYDJ5bmTyJdTva2vP52tv99wFeAL5Pq9lngnaTTDF8gNWtfRhpX/uv5Oc+T6gPg9Bz/4TmWnXPcR5Eu3fpZ4FVgjqS9mgWR3/9DgPubFBnWYp9qZGy+/d8WZcaQ6rF86uF+0n55RD/rMGsuIjx5qt1ESpbRYDqmSflhwAjgNeBzhfnzgBsLj9cBn26x3geA2aV5xwIbgA+2eN71Ob5DC/P2I335n5Yf/3ou8/lCmR2Ax4Hb8+PDcpkRLda1FLik8Pg2YE6DcuVtvwJYWCozF7htgLZ/A3BQP+/rcFJiXwMMz/M+nLf5iFLZ0/P7tV9h3o7AEuDiFusYn1/vA6X5pzXZp04p1dn0HOfOpPP5y4D5LcocTkrw/9Li/bqg258pT0N38hG91dkq0pFZcXqwb6GkT0iaI+llYD0pyY8gjWrWzM+ASyVNlbRZM2tunj4UuKl4xEc6Yt4AfLSfeF+IiI3xRcTivL6P5VmHkBLLLYUyG4Cb2XTE91TejhskTRjgJt/pwIckfQhA0rtIiWx6ftzu9j8XEQuLMyTtIOksSf8jaS3wJvBdYBdSi0wrx5BaKp4rxLIhx9NqmN+98+3KJssPZ/N9alZp+Yk5zrXA3cDTpHP+zcrMIyXzM5usb2UhJrOt5kRvdbY+IhaUpl8CSBpHGqXrLVKzfd+X9ysUmuIbmExKvv9MSiA/lXRUXrYnIOBK0pd437SW1GJQPv9a1mgY3+WkUwXk21UR8XqpzEvAOyQNizQa26fyNtwCrJQ0S9LYftZdxb2ko9O+nuiTgdfZNJRpu9v/UoN5ZwGXkn7MTCD96PmrvKzV+wRpnPQjSrG8CfxhP7H0vW65nvv8tLRPlUfA+zFpX/owsEdEjI+IRU3K/Dbwj6RTDhc0Wd/r9L+tZk251731qk+Tzn1+NiLWAiiNwT2y1ZMiYilpPOlhpKRzITAzH9339ag+l/QjomxZPzE1Om+8F5s6c70I7C5pp1KyfxewOiLeyjHeB3xK0gjgk8A/kZrG2zrPGxEh6WZSoj8v394eEa/lIu1uf6OhNH+PdPrgvL4Zkn6jYsivAPNJvdbLWvWafyXfjgR+VXFdRa9GxIKtKDMv9xk4S9IVEfFCqezIQkxmW81H9NardiEdza8vzJtCxc9ERLwVEQ+QEv2uwJiIWE1qKj6gQUvCgoh4sZ+XfY+kvmb6vlaH3wQeyrMeIh0xTyqU2SE/3uLvVxGxJiJmANcCB7ZY7xtUP2K8EThA0mdIPxw2XlBnALa/kV3Y8sj68w3ihy23YS7pNMySBrEspLkn8u24bYh3W51H2ve+UJyZf1COJnWuNNsmPqK3XjUX+CpwjaRrSB3dzgRWN3uC0pXLZgHXkb54dwH+GniBTcnhb4Af547Yt5KOCPcFPgN8KSKebhHTCtK59XNJye2i/NrXAUTEQkk3Ad/KveufIZ122B84Ncc4kXQ+eAapR/poUi/1O1us93Hg+PzcZcCyZkk5Ih6U9AxwVd62H5aKtLP9jcwGzpC0AFhM6og3tlRmCam+TpH0GvBGRPyE9O+B04G7JV2Wnz+K1GHx+Yi4vMk2PiVpBalPwb1bGe82iYhn818dT5f05fyjCdIPtBHAfZ2Iw+rJR/TWkyLiZ6Tk+AngB6TOUZOAX7Z42hrgMdJR1yxSIlkNHNvXlB4Rd5M6qO1Nai6fRbrC2rOkRN7KYuAcUivB90lN4ceVmun/OL/uNFJv+dGkfwE8kJc/SfpcX0w6D3wpcDupx3gz3yBdqe1a0hH5qf3EeROpv8CMKF04ps3tb+T8vL6vADeQOhpu1mktItaQfvAcCvw3ucNlPiVzJHAX6UfTbFLfinFsaiVp5j9Ip3c66WLg7aQfJ32OA56KiEc7HIvViCIanRYzs07KR3Pvj4jDuh2LQb6ozv3AeyJiW36gDFQcDwO3RsQl/RY2a8JH9GZmJRHxMOl0x593KwZJh5OupfDNbsVg9eBEb2bW2JlA+a9znTQSmFo4X2+2Tdx0b2ZmVmM+ojczM6sxJ3ozM7Mac6I3MzOrMSd6MzOzGnOiNzMzqzEnejMzsxr7f2Ky94aBYEWvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, _ = roc_curve(y, y_pred_proba[:,1])\n",
    "roc_auc = roc_auc_score(y, y_pred_proba[:,1])\n",
    "    \n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.title('ROC & ROC-AUC', fontsize=15)\n",
    "plt.xlabel('False positive rate (FPR)', fontsize=15)\n",
    "plt.ylabel('True positive rate (TPR)', fontsize=15)\n",
    "plt.legend(fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3B.6.4\n",
    "Постройте модель логистической регрессии при помощи sklearn без регуляризации. Чему равен $F1$-$score$ ?\n",
    "\n",
    "Ответ округлите до сотых. Пример ввода: 5.55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = pd.read_csv('./Unit_5_data/adult.data',\n",
    "                    names=['age', 'workclass', 'fnlwgt', 'education',\n",
    "                           'education-num', 'marital-status', 'occupation',\n",
    "                           'relationship', 'race', 'sex', 'capital-gain',\n",
    "                           'capital-loss', 'hours-per-week', 'native-country', 'salary'])\n",
    "\n",
    "# Избавиться от лишних признаков\n",
    "adult.drop(['native-country'], axis=1, inplace=True)\n",
    "# Сконвертировать целевой столбец в бинарные значения\n",
    "adult['salary'] = (adult['salary'] != ' <=50K').astype('int32')\n",
    "# Сделать one-hot encoding для некоторых признаков\n",
    "adult = pd.get_dummies(adult, columns=['workclass', 'education', 'marital-status', 'occupation', \n",
    "                                       'relationship', 'race', 'sex'])\n",
    "\n",
    "# Нормализовать нуждающиеся в этом признаки\n",
    "a_features = adult[['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']].values\n",
    "norm_features = (a_features - a_features.mean(axis=0)) / a_features.std(axis=0)\n",
    "adult.loc[:, ['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']] = norm_features\n",
    "\n",
    "# Разбить таблицу данных на матрицы X и y\n",
    "X = adult[list(set(adult.columns) - set(['salary']))].values\n",
    "y = adult['salary'].values\n",
    "\n",
    "# Добавить фиктивный столбец единиц (bias линейной модели)\n",
    "X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = LogisticRegression(C=10**5)\n",
    "LR.fit(X,y)\n",
    "\n",
    "y_pred = LR.predict(X)\n",
    "\n",
    "round(f1_score(y,y_pred),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3B.6.5\n",
    "Переберите коэффициенты $l2$-регуляризации от $0.01$ до $1$ с шагом $0.01$ и определите, на каком из них модель логистической регрессии из sklearn даёт наибольший $F1$-$score$ .\n",
    "\n",
    "Ответ округлите до сотых. Пример ввода: 5.55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 42.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores = pd.Series()\n",
    "for C in np.linspace(0.01,1,100):\n",
    "    LR = LogisticRegression(C=C)\n",
    "    LR.fit(X,y)\n",
    "    y_pred = LR.predict(X)\n",
    "    scores[C] = f1_score(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.91, 0.6620350877192983)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.idxmax(), scores.max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3B.6.6\n",
    "Замените в столбце `native-country` страны, у которых меньше ста записей, на `other`, поменяйте этот столбец на dummy-переменные, обучите классификатор на всей выборке и посчитайте  .\n",
    "\n",
    "Ответ округлите до сотых. Пример ввода: 5.55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ПОДВАЛ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate data\n",
    "X, y = make_blobs(n_samples=1000, random_state=42, cluster_std=5.0)\n",
    "X_train, y_train = X[:600], y[:600]\n",
    "X_valid, y_valid = X[600:800], y[600:800]\n",
    "X_train_valid, y_train_valid = X[:800], y[:800]\n",
    "X_test, y_test = X[800:], y[800:]\n",
    "\n",
    "# Train uncalibrated random forest classifier on whole train and validation\n",
    "# data and evaluate on test data\n",
    "clf = RandomForestClassifier(n_estimators=25)\n",
    "clf.fit(X_train_valid, y_train_valid)\n",
    "clf_probs = clf.predict_proba(X_test)\n",
    "score = log_loss(y_test, clf_probs)\n",
    "\n",
    "# Train random forest classifier, calibrate on validation data and evaluate\n",
    "# on test data\n",
    "clf = RandomForestClassifier(n_estimators=25)\n",
    "clf.fit(X_train, y_train)\n",
    "clf_probs = clf.predict_proba(X_test)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=\"prefit\")\n",
    "sig_clf.fit(X_valid, y_valid)\n",
    "sig_clf_probs = sig_clf.predict_proba(X_test)\n",
    "sig_score = log_loss(y_test, sig_clf_probs)\n",
    "\n",
    "# Plot changes in predicted probabilities via arrows\n",
    "plt.figure()\n",
    "colors = [\"r\", \"g\", \"b\"]\n",
    "for i in range(clf_probs.shape[0]):\n",
    "    plt.arrow(clf_probs[i, 0], clf_probs[i, 1],\n",
    "              sig_clf_probs[i, 0] - clf_probs[i, 0],\n",
    "              sig_clf_probs[i, 1] - clf_probs[i, 1],\n",
    "              color=colors[y_test[i]], head_width=1e-2)\n",
    "\n",
    "# Plot perfect predictions\n",
    "plt.plot([1.0], [0.0], 'ro', ms=20, label=\"Class 1\")\n",
    "plt.plot([0.0], [1.0], 'go', ms=20, label=\"Class 2\")\n",
    "plt.plot([0.0], [0.0], 'bo', ms=20, label=\"Class 3\")\n",
    "\n",
    "# Plot boundaries of unit simplex\n",
    "plt.plot([0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], 'k', label=\"Simplex\")\n",
    "\n",
    "# Annotate points on the simplex\n",
    "plt.annotate(r'($\\frac{1}{3}$, $\\frac{1}{3}$, $\\frac{1}{3}$)',\n",
    "             xy=(1.0/3, 1.0/3), xytext=(1.0/3, .23), xycoords='data',\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "             horizontalalignment='center', verticalalignment='center')\n",
    "plt.plot([1.0/3], [1.0/3], 'ko', ms=5)\n",
    "plt.annotate(r'($\\frac{1}{2}$, $0$, $\\frac{1}{2}$)',\n",
    "             xy=(.5, .0), xytext=(.5, .1), xycoords='data',\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "             horizontalalignment='center', verticalalignment='center')\n",
    "plt.annotate(r'($0$, $\\frac{1}{2}$, $\\frac{1}{2}$)',\n",
    "             xy=(.0, .5), xytext=(.1, .5), xycoords='data',\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "             horizontalalignment='center', verticalalignment='center')\n",
    "plt.annotate(r'($\\frac{1}{2}$, $\\frac{1}{2}$, $0$)',\n",
    "             xy=(.5, .5), xytext=(.6, .6), xycoords='data',\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "             horizontalalignment='center', verticalalignment='center')\n",
    "plt.annotate(r'($0$, $0$, $1$)',\n",
    "             xy=(0, 0), xytext=(.1, .1), xycoords='data',\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "             horizontalalignment='center', verticalalignment='center')\n",
    "plt.annotate(r'($1$, $0$, $0$)',\n",
    "             xy=(1, 0), xytext=(1, .1), xycoords='data',\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "             horizontalalignment='center', verticalalignment='center')\n",
    "plt.annotate(r'($0$, $1$, $0$)',\n",
    "             xy=(0, 1), xytext=(.1, 1), xycoords='data',\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "             horizontalalignment='center', verticalalignment='center')\n",
    "# Add grid\n",
    "plt.grid(False)\n",
    "for x in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "    plt.plot([0, x], [x, 0], 'k', alpha=0.2)\n",
    "    plt.plot([0, 0 + (1-x)/2], [x, x + (1-x)/2], 'k', alpha=0.2)\n",
    "    plt.plot([x, x + (1-x)/2], [0, 0 + (1-x)/2], 'k', alpha=0.2)\n",
    "\n",
    "plt.title(\"Change of predicted probabilities after sigmoid calibration\")\n",
    "plt.xlabel(\"Probability class 1\")\n",
    "plt.ylabel(\"Probability class 2\")\n",
    "plt.xlim(-0.05, 1.05)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "print(\"Log-loss of\")\n",
    "print(\" * uncalibrated classifier trained on 800 datapoints: %.3f \"\n",
    "      % score)\n",
    "print(\" * classifier trained on 600 datapoints and calibrated on \"\n",
    "      \"200 datapoint: %.3f\" % sig_score)\n",
    "\n",
    "# Illustrate calibrator\n",
    "plt.figure()\n",
    "# generate grid over 2-simplex\n",
    "p1d = np.linspace(0, 1, 20)\n",
    "p0, p1 = np.meshgrid(p1d, p1d)\n",
    "p2 = 1 - p0 - p1\n",
    "p = np.c_[p0.ravel(), p1.ravel(), p2.ravel()]\n",
    "p = p[p[:, 2] >= 0]\n",
    "\n",
    "calibrated_classifier = sig_clf.calibrated_classifiers_[0]\n",
    "prediction = np.vstack([calibrator.predict(this_p)\n",
    "                        for calibrator, this_p in\n",
    "                        zip(calibrated_classifier.calibrators_, p.T)]).T\n",
    "prediction /= prediction.sum(axis=1)[:, None]\n",
    "\n",
    "# Plot modifications of calibrator\n",
    "for i in range(prediction.shape[0]):\n",
    "    plt.arrow(p[i, 0], p[i, 1],\n",
    "              prediction[i, 0] - p[i, 0], prediction[i, 1] - p[i, 1],\n",
    "              head_width=1e-2, color=colors[np.argmax(p[i])])\n",
    "# Plot boundaries of unit simplex\n",
    "plt.plot([0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], 'k', label=\"Simplex\")\n",
    "\n",
    "plt.grid(False)\n",
    "for x in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "    plt.plot([0, x], [x, 0], 'k', alpha=0.2)\n",
    "    plt.plot([0, 0 + (1-x)/2], [x, x + (1-x)/2], 'k', alpha=0.2)\n",
    "    plt.plot([x, x + (1-x)/2], [0, 0 + (1-x)/2], 'k', alpha=0.2)\n",
    "\n",
    "plt.title(\"Illustration of sigmoid calibrator\")\n",
    "plt.xlabel(\"Probability class 1\")\n",
    "plt.ylabel(\"Probability class 2\")\n",
    "plt.xlim(-0.05, 1.05)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "\n",
    "plt.show()\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
