{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics \n",
    "\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "myData = pd.read_csv('./Unit_5_data/mycar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = myData.iloc[:,:-1].values\n",
    "Y = myData.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size= 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myModel = LinearRegression() #Обозначаем, что наша модель - линейная регрессия\n",
    "myModel.fit(X_train,Y_train) #обучаем модель на обучающих данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.40717852, 42.84382529, 42.84382529, 62.28047205, 11.74519046,\n",
       "       50.61848399, 46.73115464, 77.82978946, 11.74519046, 54.50581334,\n",
       "       50.61848399, 31.18183723, 58.3931427 ,  0.0832024 , 62.28047205])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = myModel.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть несколько способов посчитать ошибку, один из них — средняя квадратичная ошибка (mean squared error, MSE). Если  —  набор значений зависимой переменной, которые вернула линейная модель, — набор истинных значений зависимой переменной, то MSE считается как:  \n",
    "  \n",
    "$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y_i} - y_i)^{2}$, где $n$ — количество предсказанных значений.\n",
    "\n",
    "В общем виде: $Err(\\vec{x})= E[(y - \\hat{f}(\\vec{x}))^{2}]$\n",
    "\n",
    "\n",
    "## Разложение ошибки на Bias и Variance\n",
    "Раскроем ошибку:\n",
    "\n",
    "$Err(\\vec{x})= E[(y - \\hat{f})^{2}] =  E[{y^{2}}] + E[\\hat{f^{2}}] - 2*E[y\\hat{f}]$\n",
    "\n",
    "Посмотрим отдельно на некоторые члены этого выражения:\n",
    "\n",
    "$E[y^{2}] = Var(y) + E[y]^{2} = \\sigma^{2} + f^{2}$  \n",
    "$E[\\hat{f}^{2}] = Var(\\hat{f}) + E[\\hat{f}]^{2}$  \n",
    "$Var(x) = E[x^{2}] - E[x]^{2}$  \n",
    "$E[y\\hat{f}] = E[(f + \\varepsilon )\\hat{f}] = E[f\\hat{f}] + E[\\varepsilon \\hat{f}] = fE[\\hat{f}] + E[\\varepsilon]E[\\hat{f}] = fE[\\hat{f}]$  \n",
    "$Err(\\vec{x})= E[(y - \\hat{f})^{2}]= E[y^{2}]+E[\\hat{f}^{2}]-2*E[y\\hat{f}]=\\sigma^{2}+f^{2}+Var(\\hat{f})+E[\\hat{f}]^{2}-2fE[\\hat{f}]=$  \n",
    "$$=(f-E[\\hat{f}])^{2}+Var(\\hat{f})+\\sigma^{2}=Bias(\\hat{f})^{2} + Var(\\hat{f})+\\sigma^{2}$$  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n",
      "0.9774247946472284\n"
     ]
    }
   ],
   "source": [
    "y_happy = [4,20,110,15,23]\n",
    "y_happy_pred = [5,15,100,9,21]\n",
    "\n",
    "#Вычисляем MAE:\n",
    "\n",
    "MAE = metrics.mean_absolute_error(y_happy, y_happy_pred)\n",
    "print(MAE)\n",
    "\n",
    "#Вычисляем коэффициент детерминации:\n",
    "\n",
    "R_2 = metrics.r2_score(y_happy, y_happy_pred)\n",
    "print(R_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.3.1\n",
    "Чему равна **MSE** на этих данных?   \n",
    "\n",
    "| | | | | |\n",
    "|:-|-|-|-|-|\n",
    "|Предсказанное значение|1|3|2|5|\n",
    "|Истинное значение|2|3|-1|4|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.75"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = [1, 3, 2, 5]\n",
    "y = [2, 3, -1, 4]\n",
    "\n",
    "MSE = metrics.mean_squared_error(pred, y)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.4.2\n",
    "Найдите следующий шаг градиентного спуска. Текущая модель: $y=2x$ .  \n",
    "Обучающая выборка:  \n",
    "\n",
    "x|y\n",
    "-|-\n",
    "1|2\n",
    "2|5\n",
    "Темп обучения (learning rate):  $1/6$  \n",
    "Ответ будет выглядеть как $y = kx + b$, где $k$ и $b$ вы считаете самостоятельно.  \n",
    "Знак обыкновенной дроби записывайте через косую черту «/»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
